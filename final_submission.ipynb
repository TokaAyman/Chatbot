{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "wJ8KkzLtPbuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5311f266-60a7-4f3d-9fb4-0ad0346e58c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eUpNKZP0n5zj"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals # to print Unicode characters\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MBiAGCZ_oXWQ"
      },
      "outputs": [],
      "source": [
        "#to run on GPU\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dCiRIsdLozec"
      },
      "outputs": [],
      "source": [
        "# This function is used to print the first 'n' lines of a file.\n",
        "# 'file_path' is the path to the file and 'n' is the number of lines to print.\n",
        "def printlines(file_path, n=10):\n",
        "    # Open the file in read-binary mode.\n",
        "    with open(file_path , 'rb') as datafile:\n",
        "        # Read all the lines in the file.\n",
        "        lines = datafile.readlines()\n",
        "        # Print the total number of lines in the file.\n",
        "        print('Shape of file is {}\\n'.format(len(lines)))\n",
        "    # Loop over the first 'n' lines.\n",
        "    for line in lines[:n]:\n",
        "        # Print each line.\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load files path of the 2nd used dataset\n",
        "corpus_name = 'cornell-moviedialog-corpus'\n",
        "movie_lines_path = 'movie_lines.txt'\n",
        "movie_conversations_path = 'movie_conversations.txt'\n",
        "movie_titles_path = 'movie_titles_metadata.txt'\n",
        "movie_charaters_metadata = 'movie_characters_metadata.txt'\n",
        "printlines(movie_conversations_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHjUltVeWDMe",
        "outputId": "e381d9ae-3e30-4364-8e83-8c79ea98c1ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of file is 83097\n",
            "\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\\n\"\n",
            "b\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_33xixMHw9Fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b8846d-6d11-4eae-c464-a337748000e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lineID': 'L1045', 'characterID': 'u0', 'movieID': 'm0', 'character': 'BIANCA', 'text': 'They do not!\\n'}\n"
          ]
        }
      ],
      "source": [
        "#function to load the lines in movie_lines and defining the field per word\n",
        "def loadLines(filename , fields):\n",
        "    lines = {}\n",
        "    with open(filename , 'r' , encoding = 'iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(' +++$+++ ')\n",
        "            lineobj = {}\n",
        "            for i , field in enumerate(fields):\n",
        "                lineobj[field] = values[i]\n",
        "                lines[lineobj['lineID']] = lineobj\n",
        "    return lines\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
        "lines = loadLines(\"movie_lines.txt\", MOVIE_LINES_FIELDS)\n",
        "print(lines['L1045'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "063xsMfJxQTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a390ac-a740-449e-95bf-192ec9f811e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
              "  'lines': [{'lineID': 'L194',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
              "   {'lineID': 'L195',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
              "   {'lineID': 'L196',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
              "   {'lineID': 'L197',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L198', 'L199']\\n\",\n",
              "  'lines': [{'lineID': 'L198',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"You're asking me out.  That's so cute. What's your name again?\\n\"},\n",
              "   {'lineID': 'L199',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'Forget it.\\n'}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L200', 'L201', 'L202', 'L203']\\n\",\n",
              "  'lines': [{'lineID': 'L200',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"No, no, it's my fault -- we didn't have a proper introduction ---\\n\"},\n",
              "   {'lineID': 'L201',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'Cameron.\\n'},\n",
              "   {'lineID': 'L202',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"},\n",
              "   {'lineID': 'L203',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'Seems like she could get a date easy enough...\\n'}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L204', 'L205', 'L206']\\n\",\n",
              "  'lines': [{'lineID': 'L204',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'Why?\\n'},\n",
              "   {'lineID': 'L205',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'},\n",
              "   {'lineID': 'L206',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"That's a shame.\\n\"}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L207', 'L208']\\n\",\n",
              "  'lines': [{'lineID': 'L207',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Gosh, if only we could find Kat a boyfriend...\\n'},\n",
              "   {'lineID': 'L208',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'Let me see what I can do.\\n'}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L271', 'L272', 'L273', 'L274', 'L275']\\n\",\n",
              "  'lines': [{'lineID': 'L271',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"C'esc ma tete. This is my head\\n\"},\n",
              "   {'lineID': 'L272',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"Right.  See?  You're ready for the quiz.\\n\"},\n",
              "   {'lineID': 'L273',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\\n\"},\n",
              "   {'lineID': 'L274',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"That's because it's such a nice one.\\n\"},\n",
              "   {'lineID': 'L275',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Forget French.\\n'}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L276', 'L277']\\n\",\n",
              "  'lines': [{'lineID': 'L276',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'How is our little Find the Wench A Date plan progressing?\\n'},\n",
              "   {'lineID': 'L277',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': \"Well, there's someone I think might be --\\n\"}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L280', 'L281']\\n\",\n",
              "  'lines': [{'lineID': 'L280',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'There.\\n'},\n",
              "   {'lineID': 'L281',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': 'Where?\\n'}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L363', 'L364']\\n\",\n",
              "  'lines': [{'lineID': 'L363',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'You got something on your mind?\\n'},\n",
              "   {'lineID': 'L364',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\\n\"}]},\n",
              " {'character1ID': 'u0',\n",
              "  'character2ID': 'u2',\n",
              "  'movieID': 'm0',\n",
              "  'utteranceIDs': \"['L365', 'L366']\\n\",\n",
              "  'lines': [{'lineID': 'L365',\n",
              "    'characterID': 'u2',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'CAMERON',\n",
              "    'text': 'You have my word.  As a gentleman\\n'},\n",
              "   {'lineID': 'L366',\n",
              "    'characterID': 'u0',\n",
              "    'movieID': 'm0',\n",
              "    'character': 'BIANCA',\n",
              "    'text': \"You're sweet.\\n\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        " # Loading movie_conversations to Structure the Conversations\n",
        "def loadConversations(filename , lines , fields):\n",
        "    conversations = []\n",
        "    with open(filename , 'r' , encoding = 'iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(' +++$+++ ')\n",
        "            convObj = {}\n",
        "            for i , field in enumerate(fields):\n",
        "                convObj[field] = values[i]\n",
        "            utterance_id_pattern = re.compile('L[0-9]+')\n",
        "            lineIds = utterance_id_pattern.findall(convObj['utteranceIDs'])\n",
        "            convObj['lines'] = []\n",
        "            for lineId in lineIds:\n",
        "                convObj['lines'].append(lines[lineId])\n",
        "            conversations.append(convObj)\n",
        "    return conversations\n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "conversations = loadConversations(\"movie_conversations.txt\",lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "conversations[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "48lGPtqdxzXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8508a5fb-c600-49b0-85f5-6264214bdbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of file is 221282\n",
            "\n",
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
            "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
            "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
          ]
        }
      ],
      "source": [
        "# Extract Sentence Pairs\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        for i in range(len(conversation['lines']) - 1):\n",
        "            inputLines = conversation['lines'][i]['text'].strip()\n",
        "            targetLines = conversation['lines'][i+1]['text'].strip()\n",
        "            if inputLines and targetLines:\n",
        "                qa_pairs.append([inputLines , targetLines])\n",
        "    return qa_pairs\n",
        "# Writing File\n",
        "with open('formatted_movie_lines.txt' , 'w' , encoding = 'utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile ,lineterminator = '\\n' ,  delimiter = str(codecs.decode('\\t' , 'unicode_escape')))\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "printlines('formatted_movie_lines.txt' )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use only first 300 row from formatted_movie_lines.txt file as it is not so useful like the other datasets\n",
        "\n",
        "with open('formatted_movie_lines.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    with open('use.txt', 'w') as new_file:\n",
        "        for line in lines[:500]:\n",
        "            new_file.write(line)\n"
      ],
      "metadata": {
        "id": "I1qc9CFJqi-z"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print last row in use.txt\n",
        "\n",
        "with open('use.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    last_line = lines[-1]\n",
        "    print(last_line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLWMnk3Ftgk8",
        "outputId": "e50fe548-f26c-4fb1-9d12-753cc5ea5c57"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not asking you to swear to anything.\tI don't want you to wait for me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open all files in read mode\n",
        "with open('new.txt', 'r') as file1,open('use.txt', 'r') as file2, open('dialogs.txt', 'r') as file3:\n",
        "    # Read the contents of both files\n",
        "    content1 = file1.read()\n",
        "    content2 = file2.read()\n",
        "    content3 = file3.read()\n",
        "\n",
        "# Concatenate the contents\n",
        "concatenated_content = content1 + content2 + content3\n",
        "\n",
        "# Write the concatenated contents to a new file\n",
        "with open('concatenated.txt', 'w') as output_file:\n",
        "    output_file.write(concatenated_content)\n"
      ],
      "metadata": {
        "id": "Sn8fr4oWVb2c"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load concatenatedd.txt in data variable\n",
        "\n",
        "with open('concatenated.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# Split the data into questions and answers\n",
        "questions, answers = [], []\n",
        "for item in data:\n",
        "    parts = item.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "        questions.append(parts[0])\n",
        "        answers.append(parts[1])\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Question': questions,\n",
        "    'Answer': answers\n",
        "})\n",
        "\n",
        "# Check for missing values in the 'Answer' column\n",
        "missing_answers = df['Answer'].isnull().sum()\n",
        "\n",
        "print(f'There are {missing_answers} missing answers in the dataset.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8rcGdtmY1vR",
        "outputId": "7c64e213-d756-4adb-b1e9-1ba82aa84e92"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 missing answers in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "HWuKNsaDy3M1"
      },
      "outputs": [],
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "class Voc:\n",
        "    def __init__(self , name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\" , EOS_token : 'EOS'}\n",
        "        self.num_words = 3\n",
        "    def addSentence(self,sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self , word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    def trim(self , min_count):\n",
        "#         if self.trimmed:\n",
        "#             return\n",
        "        self.trimmed = True\n",
        "        keep_words = []\n",
        "        for k,v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\" , EOS_token : 'EOS'}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "w0dZTBnkzLbt"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 20\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD' , s) if unicodedata.category(c) !='Mn')\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "def readVocs(datafile , corpus_name):\n",
        "    lines = open(datafile , encoding = 'utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc , pairs\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKKwwEQ7zQAJ",
        "outputId": "93e70919-6a47-4b1d-83fb-3b78b8f4ca5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33027\n"
          ]
        }
      ],
      "source": [
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "def loadPrepareData(corpus , corpus_name , datafile , save_dir):\n",
        "    voc , pairs = readVocs(datafile, corpus_name)\n",
        "    pairs = filterPairs(pairs)\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(voc.num_words)\n",
        "    return voc , pairs\n",
        "voc , pairs = loadPrepareData('' , '' , 'formatted_movie_lines.txt' , '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqnTigaBzbhU",
        "outputId": "7ee1ae98-0e20-4254-f7eb-a909f94954c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['well i thought we d start with pronunciation if that s okay with you .',\n",
              "  'not the hacking and gagging and spitting part . please .'],\n",
              " ['not the hacking and gagging and spitting part . please .',\n",
              "  'okay . . . then how bout we try out some french cuisine . saturday ? night ?'],\n",
              " ['you re asking me out . that s so cute . what s your name again ?',\n",
              "  'forget it .'],\n",
              " ['no no it s my fault we didn t have a proper introduction', 'cameron .'],\n",
              " ['gosh if only we could find kat a boyfriend . . .',\n",
              "  'let me see what i can do .'],\n",
              " ['c esc ma tete . this is my head',\n",
              "  'right . see ? you re ready for the quiz .'],\n",
              " ['that s because it s such a nice one .', 'forget french .'],\n",
              " ['how is our little find the wench a date plan progressing ?',\n",
              "  'well there s someone i think might be'],\n",
              " ['there .', 'where ?'],\n",
              " ['you have my word . as a gentleman', 'you re sweet .']]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "pairs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jghaf_Vzb55",
        "outputId": "629cfc22-620c-4583-f258-6cd8f07a7006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 1570 / 2848 = 0.5513\n",
            "Trimmed from 4104 pairs to 2703 , 0.6586 of Total\n"
          ]
        }
      ],
      "source": [
        "MIN_COUNT = 3\n",
        "def trimRareWords(voc , pairs , MIN_COUNT):\n",
        "    voc.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "    print('Trimmed from {} pairs to {} , {:.4f} of Total'.format(len(pairs) , len(keep_pairs) , len(keep_pairs)/len(pairs) ))\n",
        "    return keep_pairs\n",
        "pairs = trimRareWords(voc, pairs , MIN_COUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEY69c_EzoiT",
        "outputId": "a27e309f-f55f-4f34-96ec-2e049461fb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[ 167,   21,   11,   35,   40],\n",
            "        [  45,    6,  638,  196,  343],\n",
            "        [1076,  179,   99,   92,   45],\n",
            "        [  36,  154,   47,  125,   11],\n",
            "        [1014,  234,   67,   99, 1022],\n",
            "        [  91,   47,   71,   28,   15],\n",
            "        [  36,  516,   15,    2,    2],\n",
            "        [1077,  861,    2,    0,    0],\n",
            "        [  15,   15,    0,    0,    0],\n",
            "        [   2,    2,    0,    0,    0]])\n",
            "lengths: tensor([10, 10,  8,  7,  7])\n",
            "target_variable: tensor([[  86,   18,  492,    3,    4],\n",
            "        [   4,   21,   40,   92,  243],\n",
            "        [ 167,    6,  114,   12,  331],\n",
            "        [  45,   56,  471,  678,   42],\n",
            "        [  46,  861,   15,  286, 1014],\n",
            "        [  47,   62,    2,   15,  648],\n",
            "        [1078,  111,    0,    2,   64],\n",
            "        [  15,   17,    0,    0,   14],\n",
            "        [   2,  668,    0,    0,   18],\n",
            "        [   0,   15,    0,    0,   14],\n",
            "        [   0,    2,    0,    0,  179],\n",
            "        [   0,    0,    0,    0,   55],\n",
            "        [   0,    0,    0,    0,   15],\n",
            "        [   0,    0,    0,    0,    2]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False, False,  True],\n",
            "        [ True,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False,  True]])\n",
            "max_target_len: 14\n"
          ]
        }
      ],
      "source": [
        "def indexesFromSentence(voc , sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "def zeroPadding(l , fill_value = PAD_token):\n",
        "    return list(itertools.zip_longest(*l , fillvalue = fill_value))\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def inputVar(l,voc):\n",
        "    indexes_batch = [indexesFromSentence(voc , sentence) for sentence in l] # Creating index matrix\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Lenghts of each index\n",
        "    padList = zeroPadding(indexes_batch) # Zeropadding will pad the inputs\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar , lengths\n",
        "\n",
        "def outputVar(l ,voc):\n",
        "    indexes_batch = [indexesFromSentence(voc , sentence) for sentence in l]\n",
        "    max_length = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar , mask , max_length\n",
        "def batch2TrainData(voc , pair_batch):\n",
        "    pair_batch.sort(key = lambda x : len(x[0].split(\" \")) , reverse = True)\n",
        "    input_batch , output_batch = [] , []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp , lengths = inputVar(input_batch , voc)\n",
        "    output , mask , max_target_len = outputVar(output_batch , voc)\n",
        "    return inp , lengths , output , mask , max_target_len\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Pyi9RWAq0SEr"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size , embedding , n_layers=1 , dropout = 0):\n",
        "        super(EncoderRNN , self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = dropout , bidirectional = True)\n",
        "    def forward(self , input_seq , input_lengths , hidden = None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded , input_lengths.cpu())\n",
        "        #packed = nn.utils.rnn.pack_padded_sequence(embedded , input_lengths)\n",
        "        outputs , hidden = self.gru(packed , hidden)\n",
        "        outputs , _ =  nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[: , : , :self.hidden_size] + outputs[: , : , self.hidden_size:]\n",
        "        return outputs , hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6oIFVj2u0VxB"
      },
      "outputs": [],
      "source": [
        "# Attention Layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method , hidden_size):\n",
        "        super(Attn , self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot' , 'general' , 'concat']:\n",
        "            raise ValueError(self.method , \"is not defined\")\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size , hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size*2 , hidden_size)\n",
        "            self.v = nn.Parameters(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self , hidden , encoder_output):\n",
        "        return torch.sum(hidden*encoder_output , dim= 2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      # Calculate attention energies based on the method\n",
        "      if self.method == 'general':\n",
        "          attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "      elif self.method == 'concat':\n",
        "          attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "      elif self.method == 'dot':\n",
        "          attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "      # Transpose max_length and batch_size dimensions\n",
        "      attn_energies = attn_energies.t()\n",
        "\n",
        "      # Apply softmax to normalize energies to weights in range 0 to 1, add extra dimension\n",
        "      return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wizqLGZX0flV"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self , attn_model , embedding , hidden_size , output_size , n_layers = 1 , dropout = 0.1):\n",
        "        super(LuongAttnDecoderRNN , self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size =output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = 0 , )\n",
        "        self.concat = nn.Linear(hidden_size*2 , hidden_size)\n",
        "        self.out = nn.Linear(hidden_size , output_size)\n",
        "        self.attn = Attn(attn_model , hidden_size)\n",
        "    def forward(self , input_step , last_hidden , encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output , hidden = self.gru(embedded , last_hidden)\n",
        "        attn_weights = self.attn(rnn_output , encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) #batch matrix-matrix product of matrices\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output , context) , 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output , dim = 1)\n",
        "        return output , hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9yZMfs160kR0"
      },
      "outputs": [],
      "source": [
        "# This function calculates the masked negative log likelihood loss.\n",
        "def maskNLLLoss(inp , target , mask):\n",
        "    # Sum the mask tensor to get the total number of true values (i.e., the total number of target tokens)\n",
        "    nTotal = mask.sum()\n",
        "\n",
        "    # Compute the cross entropy loss. This is done by first performing a gather operation on the input tensor\n",
        "    # using the target tensor as indices. The result is then squeezed to remove dimensions of size 1, and the\n",
        "    # logarithm is taken. The result is negated to give the negative log likelihood.\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "\n",
        "    # The cross entropy loss is then masked to ignore losses calculated for padding tokens. The masked losses\n",
        "    # are then averaged to give the mean loss.\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "\n",
        "    # The loss is moved to the device (GPU or CPU) that is being used.\n",
        "    loss = loss.to(device)\n",
        "\n",
        "    # The function returns the mean loss and the total number of target tokens.\n",
        "    return loss , nTotal.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "h7x9a5Kp07Z5"
      },
      "outputs": [],
      "source": [
        "def train(input_variable , lengths , target_variable , mask ,\n",
        "          max_target_len , encoder , decoder , embedding , encoder_optimizer ,\n",
        "          decoder_optimizer , batch_size , clip , max_length = MAX_LENGTH):\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "    encoder_outputs , encoder_hidden = encoder(input_variable , lengths)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "     # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output , decoder_hidden = decoder(decoder_input , decoder_hidden , encoder_outputs)\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1,-1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss , nTotal = maskNLLLoss(decoder_output , target_variable[t] , mask[t])\n",
        "            loss +=mask_loss\n",
        "            print_losses.append(mask_loss.item()*nTotal)\n",
        "            n_totals +=nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_ouput , decoder_hidden = decoder(decoder_input , decoder_hidden , encoder_outputs)\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _ , topi = decoder_output.topk(1)\n",
        "            # Add some randomness to next input selection\n",
        "            if random.random() < 0.1:\n",
        "              decoder_input = torch.randint(high=voc.num_words, size=(1, batch_size), dtype=torch.long, device=device)\n",
        "            else:\n",
        "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "            #decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            #decoder_input = decoder_input.to(device)\n",
        "\n",
        "            # Calculate and accumulate loss\n",
        "            #mask_loss , nTotal = maskNLLLoss(decoder_output , target_variable[t] , mask[t])\n",
        "            #loss += mask_loss\n",
        "            #print_losses.append(mask_loss.item()*nTotal)\n",
        "            #n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "# Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters() , clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters() , clip)\n",
        "\n",
        "# Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return sum(print_losses)/n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "cMbj1GaO1QkU"
      },
      "outputs": [],
      "source": [
        "# This function is used to train a sequence to sequence model\n",
        "def trainIters(model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer\n",
        "               , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iterations ,\n",
        "              batch_size , print_every,save_every , clip , corpur_name , loadFilename):\n",
        "\n",
        "    # Prepare training data\n",
        "    training_batches = [batch2TrainData(voc , [random.choice(pairs) for _ in range(batch_size)])\n",
        "                       for _ in range(n_iteration)]\n",
        "\n",
        "    print('Initializing ...')\n",
        "    start_iterations = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    # If a checkpoint is loaded, we adjust the start iterations\n",
        "    if loadFilename:\n",
        "        startiterations = checkpoint['iteration'] + 1\n",
        "\n",
        "    print('Training ...')\n",
        "\n",
        "    # Training loop\n",
        "    for iteration in range(start_iterations , n_iteration + 1):\n",
        "        # Get training batch\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        input_variable , lengths , target_variable , mask , max_target_len = training_batch\n",
        "\n",
        "        # Perform a training step\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "\n",
        "        # Accumulate loss\n",
        "        print_loss +=loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "           directory = os.path.join(save_dir , model_name , corpus_name , '{}-{}_{}'.format(encoder_n_layers , decoder_n_layers , hidden_size))\n",
        "           if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "           torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            } , os.path.join(directory, '{}_{}.tar'.format(iteration , 'checkpoint')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9N-GBcyh1tPr"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self,encoder,decoder):\n",
        "        super(GreedySearchDecoder , self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self , input_seq , input_length , max_length):\n",
        "        encoder_outputs , encoder_hidden = self.encoder(input_seq , input_length)\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        decoder_input = torch.ones(1,1,device = device , dtype = torch.long)*SOS_token\n",
        "        all_tokens = torch.zeros([0] , device = device , dtype = torch.long)\n",
        "        all_scores = torch.zeros([0] , device = device)\n",
        "        for _ in range(max_length):\n",
        "          decoder_output , decoder_hidden = self.decoder(decoder_input , decoder_hidden , encoder_outputs)\n",
        "          decoder_scores , decoder_input = torch.max(decoder_output , dim = 1)\n",
        "          all_tokens = torch.cat((all_tokens, decoder_input) , dim = 0)\n",
        "          all_scores = torch.cat((all_scores, decoder_scores) , dim = 0)\n",
        "          decoder_input = torch.unsqueeze(decoder_input , 0)\n",
        "        return all_tokens , all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qFG9C1bb17sz"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder , decoder , searcher , voc , sentence , max_length = MAX_LENGTH):\n",
        "    indexes_batch = [indexesFromSentence(voc , sentence)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n",
        "    input_batch = input_batch.to(device)\n",
        "    tokens , scores = searcher(input_batch , lengths , max_length)\n",
        "    decoder_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoder_words\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    responses = [\"I'm sorry, I did not understand you.\",\n",
        "                 \"Sorry, I can't help you with that.\",\n",
        "                 \"Can you provide me with more information to help you?\"]\n",
        "    while(1):\n",
        "      #randomly generate any of the above sentences when it does not have a match to prevent error reply\n",
        "        try:\n",
        "            input_sentence = input('> ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "             print(random.choice(responses))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEQnfsdU2Iul",
        "outputId": "00ffe47d-943d-4ad4-8926-9fd233644baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ],
      "source": [
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "save_dir = '/'\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NANK-YZd2YbM",
        "outputId": "4ee3ae46-bf13-473a-e9cd-515b65edc67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training ...\n",
            "Iteration: 1; Percent complete: 0.1%; Average loss: 7.3678\n",
            "Iteration: 2; Percent complete: 0.1%; Average loss: 7.3345\n",
            "Iteration: 3; Percent complete: 0.1%; Average loss: 7.2948\n",
            "Iteration: 4; Percent complete: 0.2%; Average loss: 7.2628\n",
            "Iteration: 5; Percent complete: 0.2%; Average loss: 7.2031\n",
            "Iteration: 6; Percent complete: 0.3%; Average loss: 7.1232\n",
            "Iteration: 7; Percent complete: 0.4%; Average loss: 7.0707\n",
            "Iteration: 8; Percent complete: 0.4%; Average loss: 6.9422\n",
            "Iteration: 9; Percent complete: 0.4%; Average loss: 6.8076\n",
            "Iteration: 10; Percent complete: 0.5%; Average loss: 6.6971\n",
            "Iteration: 11; Percent complete: 0.5%; Average loss: 6.4928\n",
            "Iteration: 12; Percent complete: 0.6%; Average loss: 6.3162\n",
            "Iteration: 13; Percent complete: 0.7%; Average loss: 6.1177\n",
            "Iteration: 14; Percent complete: 0.7%; Average loss: 6.1057\n",
            "Iteration: 15; Percent complete: 0.8%; Average loss: 6.0389\n",
            "Iteration: 16; Percent complete: 0.8%; Average loss: 5.9477\n",
            "Iteration: 17; Percent complete: 0.9%; Average loss: 5.8758\n",
            "Iteration: 18; Percent complete: 0.9%; Average loss: 5.7899\n",
            "Iteration: 19; Percent complete: 0.9%; Average loss: 5.5904\n",
            "Iteration: 20; Percent complete: 1.0%; Average loss: 5.6610\n",
            "Iteration: 21; Percent complete: 1.1%; Average loss: 5.5480\n",
            "Iteration: 22; Percent complete: 1.1%; Average loss: 5.5180\n",
            "Iteration: 23; Percent complete: 1.1%; Average loss: 5.4167\n",
            "Iteration: 24; Percent complete: 1.2%; Average loss: 5.3339\n",
            "Iteration: 25; Percent complete: 1.2%; Average loss: 5.2872\n",
            "Iteration: 26; Percent complete: 1.3%; Average loss: 5.2672\n",
            "Iteration: 27; Percent complete: 1.4%; Average loss: 5.1419\n",
            "Iteration: 28; Percent complete: 1.4%; Average loss: 5.2142\n",
            "Iteration: 29; Percent complete: 1.5%; Average loss: 5.1315\n",
            "Iteration: 30; Percent complete: 1.5%; Average loss: 5.1118\n",
            "Iteration: 31; Percent complete: 1.6%; Average loss: 5.0887\n",
            "Iteration: 32; Percent complete: 1.6%; Average loss: 4.8740\n",
            "Iteration: 33; Percent complete: 1.7%; Average loss: 5.0791\n",
            "Iteration: 34; Percent complete: 1.7%; Average loss: 5.0475\n",
            "Iteration: 35; Percent complete: 1.8%; Average loss: 4.9724\n",
            "Iteration: 36; Percent complete: 1.8%; Average loss: 4.9497\n",
            "Iteration: 37; Percent complete: 1.8%; Average loss: 4.8444\n",
            "Iteration: 38; Percent complete: 1.9%; Average loss: 4.8897\n",
            "Iteration: 39; Percent complete: 1.9%; Average loss: 4.7973\n",
            "Iteration: 40; Percent complete: 2.0%; Average loss: 4.9044\n",
            "Iteration: 41; Percent complete: 2.1%; Average loss: 4.8045\n",
            "Iteration: 42; Percent complete: 2.1%; Average loss: 4.7966\n",
            "Iteration: 43; Percent complete: 2.1%; Average loss: 4.8800\n",
            "Iteration: 44; Percent complete: 2.2%; Average loss: 5.0192\n",
            "Iteration: 45; Percent complete: 2.2%; Average loss: 4.9057\n",
            "Iteration: 46; Percent complete: 2.3%; Average loss: 4.6529\n",
            "Iteration: 47; Percent complete: 2.4%; Average loss: 4.9842\n",
            "Iteration: 48; Percent complete: 2.4%; Average loss: 4.9307\n",
            "Iteration: 49; Percent complete: 2.5%; Average loss: 4.9378\n",
            "Iteration: 50; Percent complete: 2.5%; Average loss: 4.7245\n",
            "Iteration: 51; Percent complete: 2.5%; Average loss: 4.7485\n",
            "Iteration: 52; Percent complete: 2.6%; Average loss: 4.6533\n",
            "Iteration: 53; Percent complete: 2.6%; Average loss: 4.8409\n",
            "Iteration: 54; Percent complete: 2.7%; Average loss: 4.7146\n",
            "Iteration: 55; Percent complete: 2.8%; Average loss: 4.8042\n",
            "Iteration: 56; Percent complete: 2.8%; Average loss: 4.6318\n",
            "Iteration: 57; Percent complete: 2.9%; Average loss: 4.7837\n",
            "Iteration: 58; Percent complete: 2.9%; Average loss: 4.7365\n",
            "Iteration: 59; Percent complete: 2.9%; Average loss: 4.8761\n",
            "Iteration: 60; Percent complete: 3.0%; Average loss: 4.6729\n",
            "Iteration: 61; Percent complete: 3.0%; Average loss: 4.8403\n",
            "Iteration: 62; Percent complete: 3.1%; Average loss: 4.6332\n",
            "Iteration: 63; Percent complete: 3.1%; Average loss: 4.6537\n",
            "Iteration: 64; Percent complete: 3.2%; Average loss: 4.7215\n",
            "Iteration: 65; Percent complete: 3.2%; Average loss: 4.6425\n",
            "Iteration: 66; Percent complete: 3.3%; Average loss: 4.7410\n",
            "Iteration: 67; Percent complete: 3.4%; Average loss: 4.8875\n",
            "Iteration: 68; Percent complete: 3.4%; Average loss: 4.5669\n",
            "Iteration: 69; Percent complete: 3.5%; Average loss: 4.5286\n",
            "Iteration: 70; Percent complete: 3.5%; Average loss: 4.6428\n",
            "Iteration: 71; Percent complete: 3.5%; Average loss: 4.8563\n",
            "Iteration: 72; Percent complete: 3.6%; Average loss: 4.8166\n",
            "Iteration: 73; Percent complete: 3.6%; Average loss: 4.5231\n",
            "Iteration: 74; Percent complete: 3.7%; Average loss: 4.8162\n",
            "Iteration: 75; Percent complete: 3.8%; Average loss: 4.7778\n",
            "Iteration: 76; Percent complete: 3.8%; Average loss: 4.6724\n",
            "Iteration: 77; Percent complete: 3.9%; Average loss: 4.7184\n",
            "Iteration: 78; Percent complete: 3.9%; Average loss: 4.6719\n",
            "Iteration: 79; Percent complete: 4.0%; Average loss: 4.6195\n",
            "Iteration: 80; Percent complete: 4.0%; Average loss: 4.7102\n",
            "Iteration: 81; Percent complete: 4.0%; Average loss: 4.7174\n",
            "Iteration: 82; Percent complete: 4.1%; Average loss: 4.6381\n",
            "Iteration: 83; Percent complete: 4.2%; Average loss: 4.7491\n",
            "Iteration: 84; Percent complete: 4.2%; Average loss: 4.6614\n",
            "Iteration: 85; Percent complete: 4.2%; Average loss: 4.7654\n",
            "Iteration: 86; Percent complete: 4.3%; Average loss: 4.5726\n",
            "Iteration: 87; Percent complete: 4.3%; Average loss: 4.4816\n",
            "Iteration: 88; Percent complete: 4.4%; Average loss: 4.5736\n",
            "Iteration: 89; Percent complete: 4.5%; Average loss: 4.6627\n",
            "Iteration: 90; Percent complete: 4.5%; Average loss: 4.5608\n",
            "Iteration: 91; Percent complete: 4.5%; Average loss: 4.6161\n",
            "Iteration: 92; Percent complete: 4.6%; Average loss: 4.5850\n",
            "Iteration: 93; Percent complete: 4.7%; Average loss: 4.5870\n",
            "Iteration: 94; Percent complete: 4.7%; Average loss: 4.5495\n",
            "Iteration: 95; Percent complete: 4.8%; Average loss: 4.5829\n",
            "Iteration: 96; Percent complete: 4.8%; Average loss: 4.4438\n",
            "Iteration: 97; Percent complete: 4.9%; Average loss: 4.4614\n",
            "Iteration: 98; Percent complete: 4.9%; Average loss: 4.6520\n",
            "Iteration: 99; Percent complete: 5.0%; Average loss: 4.5717\n",
            "Iteration: 100; Percent complete: 5.0%; Average loss: 4.5003\n",
            "Iteration: 101; Percent complete: 5.1%; Average loss: 4.5664\n",
            "Iteration: 102; Percent complete: 5.1%; Average loss: 4.5005\n",
            "Iteration: 103; Percent complete: 5.1%; Average loss: 4.4013\n",
            "Iteration: 104; Percent complete: 5.2%; Average loss: 4.4999\n",
            "Iteration: 105; Percent complete: 5.2%; Average loss: 4.3903\n",
            "Iteration: 106; Percent complete: 5.3%; Average loss: 4.5935\n",
            "Iteration: 107; Percent complete: 5.3%; Average loss: 4.5068\n",
            "Iteration: 108; Percent complete: 5.4%; Average loss: 4.4140\n",
            "Iteration: 109; Percent complete: 5.5%; Average loss: 4.3144\n",
            "Iteration: 110; Percent complete: 5.5%; Average loss: 4.5387\n",
            "Iteration: 111; Percent complete: 5.5%; Average loss: 4.4534\n",
            "Iteration: 112; Percent complete: 5.6%; Average loss: 4.5104\n",
            "Iteration: 113; Percent complete: 5.7%; Average loss: 4.5233\n",
            "Iteration: 114; Percent complete: 5.7%; Average loss: 4.5040\n",
            "Iteration: 115; Percent complete: 5.8%; Average loss: 4.5191\n",
            "Iteration: 116; Percent complete: 5.8%; Average loss: 4.3980\n",
            "Iteration: 117; Percent complete: 5.9%; Average loss: 4.4193\n",
            "Iteration: 118; Percent complete: 5.9%; Average loss: 4.4600\n",
            "Iteration: 119; Percent complete: 5.9%; Average loss: 4.4568\n",
            "Iteration: 120; Percent complete: 6.0%; Average loss: 4.4035\n",
            "Iteration: 121; Percent complete: 6.0%; Average loss: 4.4386\n",
            "Iteration: 122; Percent complete: 6.1%; Average loss: 4.5432\n",
            "Iteration: 123; Percent complete: 6.2%; Average loss: 4.3933\n",
            "Iteration: 124; Percent complete: 6.2%; Average loss: 4.6169\n",
            "Iteration: 125; Percent complete: 6.2%; Average loss: 4.5477\n",
            "Iteration: 126; Percent complete: 6.3%; Average loss: 4.5181\n",
            "Iteration: 127; Percent complete: 6.3%; Average loss: 4.4257\n",
            "Iteration: 128; Percent complete: 6.4%; Average loss: 4.4013\n",
            "Iteration: 129; Percent complete: 6.5%; Average loss: 4.3819\n",
            "Iteration: 130; Percent complete: 6.5%; Average loss: 4.4377\n",
            "Iteration: 131; Percent complete: 6.6%; Average loss: 4.3326\n",
            "Iteration: 132; Percent complete: 6.6%; Average loss: 4.4482\n",
            "Iteration: 133; Percent complete: 6.7%; Average loss: 4.2474\n",
            "Iteration: 134; Percent complete: 6.7%; Average loss: 4.3366\n",
            "Iteration: 135; Percent complete: 6.8%; Average loss: 4.4288\n",
            "Iteration: 136; Percent complete: 6.8%; Average loss: 4.2952\n",
            "Iteration: 137; Percent complete: 6.9%; Average loss: 4.2399\n",
            "Iteration: 138; Percent complete: 6.9%; Average loss: 4.4634\n",
            "Iteration: 139; Percent complete: 7.0%; Average loss: 4.4177\n",
            "Iteration: 140; Percent complete: 7.0%; Average loss: 4.4647\n",
            "Iteration: 141; Percent complete: 7.0%; Average loss: 4.5519\n",
            "Iteration: 142; Percent complete: 7.1%; Average loss: 4.3349\n",
            "Iteration: 143; Percent complete: 7.1%; Average loss: 4.2115\n",
            "Iteration: 144; Percent complete: 7.2%; Average loss: 4.4366\n",
            "Iteration: 145; Percent complete: 7.2%; Average loss: 4.3113\n",
            "Iteration: 146; Percent complete: 7.3%; Average loss: 4.3483\n",
            "Iteration: 147; Percent complete: 7.3%; Average loss: 4.3915\n",
            "Iteration: 148; Percent complete: 7.4%; Average loss: 4.3123\n",
            "Iteration: 149; Percent complete: 7.4%; Average loss: 4.4064\n",
            "Iteration: 150; Percent complete: 7.5%; Average loss: 4.4027\n",
            "Iteration: 151; Percent complete: 7.5%; Average loss: 4.3096\n",
            "Iteration: 152; Percent complete: 7.6%; Average loss: 4.2322\n",
            "Iteration: 153; Percent complete: 7.6%; Average loss: 4.1628\n",
            "Iteration: 154; Percent complete: 7.7%; Average loss: 4.2675\n",
            "Iteration: 155; Percent complete: 7.8%; Average loss: 4.3680\n",
            "Iteration: 156; Percent complete: 7.8%; Average loss: 4.3895\n",
            "Iteration: 157; Percent complete: 7.8%; Average loss: 4.0521\n",
            "Iteration: 158; Percent complete: 7.9%; Average loss: 4.4107\n",
            "Iteration: 159; Percent complete: 8.0%; Average loss: 4.2359\n",
            "Iteration: 160; Percent complete: 8.0%; Average loss: 4.3119\n",
            "Iteration: 161; Percent complete: 8.1%; Average loss: 4.3328\n",
            "Iteration: 162; Percent complete: 8.1%; Average loss: 4.2039\n",
            "Iteration: 163; Percent complete: 8.2%; Average loss: 4.2530\n",
            "Iteration: 164; Percent complete: 8.2%; Average loss: 4.3343\n",
            "Iteration: 165; Percent complete: 8.2%; Average loss: 4.4197\n",
            "Iteration: 166; Percent complete: 8.3%; Average loss: 4.2092\n",
            "Iteration: 167; Percent complete: 8.3%; Average loss: 4.3521\n",
            "Iteration: 168; Percent complete: 8.4%; Average loss: 4.3507\n",
            "Iteration: 169; Percent complete: 8.5%; Average loss: 4.1465\n",
            "Iteration: 170; Percent complete: 8.5%; Average loss: 4.3031\n",
            "Iteration: 171; Percent complete: 8.6%; Average loss: 4.3144\n",
            "Iteration: 172; Percent complete: 8.6%; Average loss: 4.1585\n",
            "Iteration: 173; Percent complete: 8.6%; Average loss: 4.2059\n",
            "Iteration: 174; Percent complete: 8.7%; Average loss: 4.4041\n",
            "Iteration: 175; Percent complete: 8.8%; Average loss: 4.1441\n",
            "Iteration: 176; Percent complete: 8.8%; Average loss: 4.1646\n",
            "Iteration: 177; Percent complete: 8.8%; Average loss: 4.2769\n",
            "Iteration: 178; Percent complete: 8.9%; Average loss: 4.2423\n",
            "Iteration: 179; Percent complete: 8.9%; Average loss: 4.2468\n",
            "Iteration: 180; Percent complete: 9.0%; Average loss: 4.2425\n",
            "Iteration: 181; Percent complete: 9.0%; Average loss: 4.3172\n",
            "Iteration: 182; Percent complete: 9.1%; Average loss: 4.2953\n",
            "Iteration: 183; Percent complete: 9.2%; Average loss: 4.2922\n",
            "Iteration: 184; Percent complete: 9.2%; Average loss: 4.2485\n",
            "Iteration: 185; Percent complete: 9.2%; Average loss: 4.0599\n",
            "Iteration: 186; Percent complete: 9.3%; Average loss: 4.2218\n",
            "Iteration: 187; Percent complete: 9.3%; Average loss: 4.0993\n",
            "Iteration: 188; Percent complete: 9.4%; Average loss: 4.1875\n",
            "Iteration: 189; Percent complete: 9.4%; Average loss: 4.1406\n",
            "Iteration: 190; Percent complete: 9.5%; Average loss: 4.1951\n",
            "Iteration: 191; Percent complete: 9.6%; Average loss: 4.1871\n",
            "Iteration: 192; Percent complete: 9.6%; Average loss: 4.1007\n",
            "Iteration: 193; Percent complete: 9.7%; Average loss: 4.1831\n",
            "Iteration: 194; Percent complete: 9.7%; Average loss: 4.1296\n",
            "Iteration: 195; Percent complete: 9.8%; Average loss: 4.0628\n",
            "Iteration: 196; Percent complete: 9.8%; Average loss: 4.1869\n",
            "Iteration: 197; Percent complete: 9.8%; Average loss: 4.1205\n",
            "Iteration: 198; Percent complete: 9.9%; Average loss: 4.2330\n",
            "Iteration: 199; Percent complete: 10.0%; Average loss: 4.1021\n",
            "Iteration: 200; Percent complete: 10.0%; Average loss: 4.0816\n",
            "Iteration: 201; Percent complete: 10.1%; Average loss: 4.2473\n",
            "Iteration: 202; Percent complete: 10.1%; Average loss: 4.2984\n",
            "Iteration: 203; Percent complete: 10.2%; Average loss: 4.1369\n",
            "Iteration: 204; Percent complete: 10.2%; Average loss: 4.1139\n",
            "Iteration: 205; Percent complete: 10.2%; Average loss: 4.0929\n",
            "Iteration: 206; Percent complete: 10.3%; Average loss: 4.0582\n",
            "Iteration: 207; Percent complete: 10.3%; Average loss: 3.9685\n",
            "Iteration: 208; Percent complete: 10.4%; Average loss: 3.9780\n",
            "Iteration: 209; Percent complete: 10.4%; Average loss: 4.0116\n",
            "Iteration: 210; Percent complete: 10.5%; Average loss: 4.0708\n",
            "Iteration: 211; Percent complete: 10.5%; Average loss: 3.9188\n",
            "Iteration: 212; Percent complete: 10.6%; Average loss: 4.1100\n",
            "Iteration: 213; Percent complete: 10.7%; Average loss: 4.1366\n",
            "Iteration: 214; Percent complete: 10.7%; Average loss: 4.1139\n",
            "Iteration: 215; Percent complete: 10.8%; Average loss: 4.1510\n",
            "Iteration: 216; Percent complete: 10.8%; Average loss: 3.9954\n",
            "Iteration: 217; Percent complete: 10.8%; Average loss: 3.9136\n",
            "Iteration: 218; Percent complete: 10.9%; Average loss: 3.9583\n",
            "Iteration: 219; Percent complete: 10.9%; Average loss: 3.9415\n",
            "Iteration: 220; Percent complete: 11.0%; Average loss: 4.0751\n",
            "Iteration: 221; Percent complete: 11.1%; Average loss: 4.0312\n",
            "Iteration: 222; Percent complete: 11.1%; Average loss: 4.0505\n",
            "Iteration: 223; Percent complete: 11.2%; Average loss: 4.0764\n",
            "Iteration: 224; Percent complete: 11.2%; Average loss: 4.3043\n",
            "Iteration: 225; Percent complete: 11.2%; Average loss: 3.9805\n",
            "Iteration: 226; Percent complete: 11.3%; Average loss: 4.0422\n",
            "Iteration: 227; Percent complete: 11.3%; Average loss: 4.0058\n",
            "Iteration: 228; Percent complete: 11.4%; Average loss: 4.0264\n",
            "Iteration: 229; Percent complete: 11.5%; Average loss: 4.0752\n",
            "Iteration: 230; Percent complete: 11.5%; Average loss: 3.9670\n",
            "Iteration: 231; Percent complete: 11.6%; Average loss: 4.0385\n",
            "Iteration: 232; Percent complete: 11.6%; Average loss: 4.0109\n",
            "Iteration: 233; Percent complete: 11.7%; Average loss: 4.1884\n",
            "Iteration: 234; Percent complete: 11.7%; Average loss: 4.0776\n",
            "Iteration: 235; Percent complete: 11.8%; Average loss: 4.0770\n",
            "Iteration: 236; Percent complete: 11.8%; Average loss: 3.8359\n",
            "Iteration: 237; Percent complete: 11.8%; Average loss: 3.9377\n",
            "Iteration: 238; Percent complete: 11.9%; Average loss: 4.1628\n",
            "Iteration: 239; Percent complete: 11.9%; Average loss: 4.0390\n",
            "Iteration: 240; Percent complete: 12.0%; Average loss: 4.1274\n",
            "Iteration: 241; Percent complete: 12.0%; Average loss: 3.9655\n",
            "Iteration: 242; Percent complete: 12.1%; Average loss: 3.9220\n",
            "Iteration: 243; Percent complete: 12.2%; Average loss: 3.9762\n",
            "Iteration: 244; Percent complete: 12.2%; Average loss: 4.1396\n",
            "Iteration: 245; Percent complete: 12.2%; Average loss: 4.1389\n",
            "Iteration: 246; Percent complete: 12.3%; Average loss: 3.8997\n",
            "Iteration: 247; Percent complete: 12.3%; Average loss: 4.0031\n",
            "Iteration: 248; Percent complete: 12.4%; Average loss: 4.0602\n",
            "Iteration: 249; Percent complete: 12.4%; Average loss: 4.1057\n",
            "Iteration: 250; Percent complete: 12.5%; Average loss: 3.9890\n",
            "Iteration: 251; Percent complete: 12.6%; Average loss: 3.8955\n",
            "Iteration: 252; Percent complete: 12.6%; Average loss: 3.9501\n",
            "Iteration: 253; Percent complete: 12.7%; Average loss: 3.9407\n",
            "Iteration: 254; Percent complete: 12.7%; Average loss: 3.8559\n",
            "Iteration: 255; Percent complete: 12.8%; Average loss: 3.9854\n",
            "Iteration: 256; Percent complete: 12.8%; Average loss: 3.9729\n",
            "Iteration: 257; Percent complete: 12.8%; Average loss: 4.0357\n",
            "Iteration: 258; Percent complete: 12.9%; Average loss: 3.8536\n",
            "Iteration: 259; Percent complete: 13.0%; Average loss: 3.9998\n",
            "Iteration: 260; Percent complete: 13.0%; Average loss: 3.8593\n",
            "Iteration: 261; Percent complete: 13.1%; Average loss: 3.8576\n",
            "Iteration: 262; Percent complete: 13.1%; Average loss: 3.7825\n",
            "Iteration: 263; Percent complete: 13.2%; Average loss: 3.9324\n",
            "Iteration: 264; Percent complete: 13.2%; Average loss: 3.7670\n",
            "Iteration: 265; Percent complete: 13.2%; Average loss: 3.9597\n",
            "Iteration: 266; Percent complete: 13.3%; Average loss: 3.9879\n",
            "Iteration: 267; Percent complete: 13.4%; Average loss: 3.8773\n",
            "Iteration: 268; Percent complete: 13.4%; Average loss: 4.0599\n",
            "Iteration: 269; Percent complete: 13.5%; Average loss: 3.9698\n",
            "Iteration: 270; Percent complete: 13.5%; Average loss: 3.8971\n",
            "Iteration: 271; Percent complete: 13.6%; Average loss: 3.9989\n",
            "Iteration: 272; Percent complete: 13.6%; Average loss: 3.9303\n",
            "Iteration: 273; Percent complete: 13.7%; Average loss: 3.8399\n",
            "Iteration: 274; Percent complete: 13.7%; Average loss: 4.0402\n",
            "Iteration: 275; Percent complete: 13.8%; Average loss: 4.0320\n",
            "Iteration: 276; Percent complete: 13.8%; Average loss: 4.0013\n",
            "Iteration: 277; Percent complete: 13.9%; Average loss: 3.7685\n",
            "Iteration: 278; Percent complete: 13.9%; Average loss: 3.8756\n",
            "Iteration: 279; Percent complete: 14.0%; Average loss: 3.7707\n",
            "Iteration: 280; Percent complete: 14.0%; Average loss: 3.8293\n",
            "Iteration: 281; Percent complete: 14.1%; Average loss: 3.9895\n",
            "Iteration: 282; Percent complete: 14.1%; Average loss: 4.0523\n",
            "Iteration: 283; Percent complete: 14.1%; Average loss: 3.8840\n",
            "Iteration: 284; Percent complete: 14.2%; Average loss: 3.7813\n",
            "Iteration: 285; Percent complete: 14.2%; Average loss: 3.8691\n",
            "Iteration: 286; Percent complete: 14.3%; Average loss: 3.9048\n",
            "Iteration: 287; Percent complete: 14.3%; Average loss: 3.8014\n",
            "Iteration: 288; Percent complete: 14.4%; Average loss: 3.7488\n",
            "Iteration: 289; Percent complete: 14.4%; Average loss: 3.8148\n",
            "Iteration: 290; Percent complete: 14.5%; Average loss: 4.0056\n",
            "Iteration: 291; Percent complete: 14.5%; Average loss: 3.7666\n",
            "Iteration: 292; Percent complete: 14.6%; Average loss: 3.8690\n",
            "Iteration: 293; Percent complete: 14.6%; Average loss: 3.8330\n",
            "Iteration: 294; Percent complete: 14.7%; Average loss: 3.7816\n",
            "Iteration: 295; Percent complete: 14.8%; Average loss: 3.7985\n",
            "Iteration: 296; Percent complete: 14.8%; Average loss: 3.7966\n",
            "Iteration: 297; Percent complete: 14.8%; Average loss: 3.7890\n",
            "Iteration: 298; Percent complete: 14.9%; Average loss: 3.6340\n",
            "Iteration: 299; Percent complete: 14.9%; Average loss: 3.9209\n",
            "Iteration: 300; Percent complete: 15.0%; Average loss: 3.8664\n",
            "Iteration: 301; Percent complete: 15.0%; Average loss: 3.8744\n",
            "Iteration: 302; Percent complete: 15.1%; Average loss: 3.8692\n",
            "Iteration: 303; Percent complete: 15.2%; Average loss: 3.8739\n",
            "Iteration: 304; Percent complete: 15.2%; Average loss: 3.9035\n",
            "Iteration: 305; Percent complete: 15.2%; Average loss: 3.6405\n",
            "Iteration: 306; Percent complete: 15.3%; Average loss: 3.7842\n",
            "Iteration: 307; Percent complete: 15.3%; Average loss: 3.8250\n",
            "Iteration: 308; Percent complete: 15.4%; Average loss: 3.8390\n",
            "Iteration: 309; Percent complete: 15.4%; Average loss: 3.7783\n",
            "Iteration: 310; Percent complete: 15.5%; Average loss: 3.8996\n",
            "Iteration: 311; Percent complete: 15.6%; Average loss: 3.8969\n",
            "Iteration: 312; Percent complete: 15.6%; Average loss: 3.6847\n",
            "Iteration: 313; Percent complete: 15.7%; Average loss: 3.7330\n",
            "Iteration: 314; Percent complete: 15.7%; Average loss: 3.8284\n",
            "Iteration: 315; Percent complete: 15.8%; Average loss: 3.8296\n",
            "Iteration: 316; Percent complete: 15.8%; Average loss: 3.8987\n",
            "Iteration: 317; Percent complete: 15.8%; Average loss: 3.5378\n",
            "Iteration: 318; Percent complete: 15.9%; Average loss: 3.8055\n",
            "Iteration: 319; Percent complete: 16.0%; Average loss: 3.6395\n",
            "Iteration: 320; Percent complete: 16.0%; Average loss: 3.8166\n",
            "Iteration: 321; Percent complete: 16.1%; Average loss: 3.8569\n",
            "Iteration: 322; Percent complete: 16.1%; Average loss: 3.9042\n",
            "Iteration: 323; Percent complete: 16.2%; Average loss: 3.6359\n",
            "Iteration: 324; Percent complete: 16.2%; Average loss: 3.7821\n",
            "Iteration: 325; Percent complete: 16.2%; Average loss: 3.5896\n",
            "Iteration: 326; Percent complete: 16.3%; Average loss: 3.9075\n",
            "Iteration: 327; Percent complete: 16.4%; Average loss: 3.7358\n",
            "Iteration: 328; Percent complete: 16.4%; Average loss: 3.7475\n",
            "Iteration: 329; Percent complete: 16.4%; Average loss: 3.7626\n",
            "Iteration: 330; Percent complete: 16.5%; Average loss: 3.6921\n",
            "Iteration: 331; Percent complete: 16.6%; Average loss: 3.6557\n",
            "Iteration: 332; Percent complete: 16.6%; Average loss: 4.0721\n",
            "Iteration: 333; Percent complete: 16.7%; Average loss: 3.6382\n",
            "Iteration: 334; Percent complete: 16.7%; Average loss: 3.6969\n",
            "Iteration: 335; Percent complete: 16.8%; Average loss: 3.7371\n",
            "Iteration: 336; Percent complete: 16.8%; Average loss: 3.6767\n",
            "Iteration: 337; Percent complete: 16.9%; Average loss: 3.6867\n",
            "Iteration: 338; Percent complete: 16.9%; Average loss: 3.7280\n",
            "Iteration: 339; Percent complete: 17.0%; Average loss: 3.7848\n",
            "Iteration: 340; Percent complete: 17.0%; Average loss: 3.6537\n",
            "Iteration: 341; Percent complete: 17.1%; Average loss: 3.7775\n",
            "Iteration: 342; Percent complete: 17.1%; Average loss: 3.7461\n",
            "Iteration: 343; Percent complete: 17.2%; Average loss: 3.7783\n",
            "Iteration: 344; Percent complete: 17.2%; Average loss: 3.6448\n",
            "Iteration: 345; Percent complete: 17.2%; Average loss: 3.6520\n",
            "Iteration: 346; Percent complete: 17.3%; Average loss: 3.7039\n",
            "Iteration: 347; Percent complete: 17.3%; Average loss: 3.7737\n",
            "Iteration: 348; Percent complete: 17.4%; Average loss: 3.5759\n",
            "Iteration: 349; Percent complete: 17.4%; Average loss: 3.6560\n",
            "Iteration: 350; Percent complete: 17.5%; Average loss: 3.5660\n",
            "Iteration: 351; Percent complete: 17.5%; Average loss: 3.5729\n",
            "Iteration: 352; Percent complete: 17.6%; Average loss: 3.6410\n",
            "Iteration: 353; Percent complete: 17.6%; Average loss: 3.9567\n",
            "Iteration: 354; Percent complete: 17.7%; Average loss: 3.4826\n",
            "Iteration: 355; Percent complete: 17.8%; Average loss: 3.8183\n",
            "Iteration: 356; Percent complete: 17.8%; Average loss: 3.6751\n",
            "Iteration: 357; Percent complete: 17.8%; Average loss: 3.6864\n",
            "Iteration: 358; Percent complete: 17.9%; Average loss: 3.7377\n",
            "Iteration: 359; Percent complete: 17.9%; Average loss: 3.7755\n",
            "Iteration: 360; Percent complete: 18.0%; Average loss: 3.7886\n",
            "Iteration: 361; Percent complete: 18.1%; Average loss: 3.7443\n",
            "Iteration: 362; Percent complete: 18.1%; Average loss: 3.6435\n",
            "Iteration: 363; Percent complete: 18.1%; Average loss: 3.6544\n",
            "Iteration: 364; Percent complete: 18.2%; Average loss: 3.6748\n",
            "Iteration: 365; Percent complete: 18.2%; Average loss: 3.8485\n",
            "Iteration: 366; Percent complete: 18.3%; Average loss: 3.7418\n",
            "Iteration: 367; Percent complete: 18.4%; Average loss: 3.5391\n",
            "Iteration: 368; Percent complete: 18.4%; Average loss: 3.6497\n",
            "Iteration: 369; Percent complete: 18.4%; Average loss: 3.7016\n",
            "Iteration: 370; Percent complete: 18.5%; Average loss: 3.7324\n",
            "Iteration: 371; Percent complete: 18.6%; Average loss: 3.6096\n",
            "Iteration: 372; Percent complete: 18.6%; Average loss: 3.7344\n",
            "Iteration: 373; Percent complete: 18.6%; Average loss: 3.7507\n",
            "Iteration: 374; Percent complete: 18.7%; Average loss: 3.5561\n",
            "Iteration: 375; Percent complete: 18.8%; Average loss: 3.7470\n",
            "Iteration: 376; Percent complete: 18.8%; Average loss: 3.5770\n",
            "Iteration: 377; Percent complete: 18.9%; Average loss: 3.6672\n",
            "Iteration: 378; Percent complete: 18.9%; Average loss: 3.6580\n",
            "Iteration: 379; Percent complete: 18.9%; Average loss: 3.7362\n",
            "Iteration: 380; Percent complete: 19.0%; Average loss: 3.7639\n",
            "Iteration: 381; Percent complete: 19.1%; Average loss: 3.7033\n",
            "Iteration: 382; Percent complete: 19.1%; Average loss: 3.6848\n",
            "Iteration: 383; Percent complete: 19.1%; Average loss: 3.6710\n",
            "Iteration: 384; Percent complete: 19.2%; Average loss: 3.6412\n",
            "Iteration: 385; Percent complete: 19.2%; Average loss: 3.4579\n",
            "Iteration: 386; Percent complete: 19.3%; Average loss: 3.5063\n",
            "Iteration: 387; Percent complete: 19.4%; Average loss: 3.6498\n",
            "Iteration: 388; Percent complete: 19.4%; Average loss: 3.4787\n",
            "Iteration: 389; Percent complete: 19.4%; Average loss: 3.4495\n",
            "Iteration: 390; Percent complete: 19.5%; Average loss: 3.6804\n",
            "Iteration: 391; Percent complete: 19.6%; Average loss: 3.5541\n",
            "Iteration: 392; Percent complete: 19.6%; Average loss: 3.7342\n",
            "Iteration: 393; Percent complete: 19.7%; Average loss: 3.4260\n",
            "Iteration: 394; Percent complete: 19.7%; Average loss: 3.4817\n",
            "Iteration: 395; Percent complete: 19.8%; Average loss: 3.5563\n",
            "Iteration: 396; Percent complete: 19.8%; Average loss: 3.5108\n",
            "Iteration: 397; Percent complete: 19.9%; Average loss: 3.4532\n",
            "Iteration: 398; Percent complete: 19.9%; Average loss: 3.7032\n",
            "Iteration: 399; Percent complete: 20.0%; Average loss: 3.7311\n",
            "Iteration: 400; Percent complete: 20.0%; Average loss: 3.6102\n",
            "Iteration: 401; Percent complete: 20.1%; Average loss: 3.5131\n",
            "Iteration: 402; Percent complete: 20.1%; Average loss: 3.6205\n",
            "Iteration: 403; Percent complete: 20.2%; Average loss: 3.5454\n",
            "Iteration: 404; Percent complete: 20.2%; Average loss: 3.4826\n",
            "Iteration: 405; Percent complete: 20.2%; Average loss: 3.5430\n",
            "Iteration: 406; Percent complete: 20.3%; Average loss: 3.5786\n",
            "Iteration: 407; Percent complete: 20.3%; Average loss: 3.4492\n",
            "Iteration: 408; Percent complete: 20.4%; Average loss: 3.5076\n",
            "Iteration: 409; Percent complete: 20.4%; Average loss: 3.3898\n",
            "Iteration: 410; Percent complete: 20.5%; Average loss: 3.3362\n",
            "Iteration: 411; Percent complete: 20.5%; Average loss: 3.4304\n",
            "Iteration: 412; Percent complete: 20.6%; Average loss: 3.2763\n",
            "Iteration: 413; Percent complete: 20.6%; Average loss: 3.3737\n",
            "Iteration: 414; Percent complete: 20.7%; Average loss: 3.6648\n",
            "Iteration: 415; Percent complete: 20.8%; Average loss: 3.5264\n",
            "Iteration: 416; Percent complete: 20.8%; Average loss: 3.5806\n",
            "Iteration: 417; Percent complete: 20.8%; Average loss: 3.4990\n",
            "Iteration: 418; Percent complete: 20.9%; Average loss: 3.3845\n",
            "Iteration: 419; Percent complete: 20.9%; Average loss: 3.3115\n",
            "Iteration: 420; Percent complete: 21.0%; Average loss: 3.5872\n",
            "Iteration: 421; Percent complete: 21.1%; Average loss: 3.5954\n",
            "Iteration: 422; Percent complete: 21.1%; Average loss: 3.6195\n",
            "Iteration: 423; Percent complete: 21.1%; Average loss: 3.6206\n",
            "Iteration: 424; Percent complete: 21.2%; Average loss: 3.3653\n",
            "Iteration: 425; Percent complete: 21.2%; Average loss: 3.5270\n",
            "Iteration: 426; Percent complete: 21.3%; Average loss: 3.3938\n",
            "Iteration: 427; Percent complete: 21.3%; Average loss: 3.4319\n",
            "Iteration: 428; Percent complete: 21.4%; Average loss: 3.6532\n",
            "Iteration: 429; Percent complete: 21.4%; Average loss: 3.5316\n",
            "Iteration: 430; Percent complete: 21.5%; Average loss: 3.5342\n",
            "Iteration: 431; Percent complete: 21.6%; Average loss: 3.4977\n",
            "Iteration: 432; Percent complete: 21.6%; Average loss: 3.7362\n",
            "Iteration: 433; Percent complete: 21.6%; Average loss: 3.5892\n",
            "Iteration: 434; Percent complete: 21.7%; Average loss: 3.5560\n",
            "Iteration: 435; Percent complete: 21.8%; Average loss: 3.5113\n",
            "Iteration: 436; Percent complete: 21.8%; Average loss: 3.5521\n",
            "Iteration: 437; Percent complete: 21.9%; Average loss: 3.6526\n",
            "Iteration: 438; Percent complete: 21.9%; Average loss: 3.3882\n",
            "Iteration: 439; Percent complete: 21.9%; Average loss: 3.5136\n",
            "Iteration: 440; Percent complete: 22.0%; Average loss: 3.4350\n",
            "Iteration: 441; Percent complete: 22.1%; Average loss: 3.6295\n",
            "Iteration: 442; Percent complete: 22.1%; Average loss: 3.5835\n",
            "Iteration: 443; Percent complete: 22.1%; Average loss: 3.3440\n",
            "Iteration: 444; Percent complete: 22.2%; Average loss: 3.4575\n",
            "Iteration: 445; Percent complete: 22.2%; Average loss: 3.5790\n",
            "Iteration: 446; Percent complete: 22.3%; Average loss: 3.3684\n",
            "Iteration: 447; Percent complete: 22.4%; Average loss: 3.3960\n",
            "Iteration: 448; Percent complete: 22.4%; Average loss: 3.5986\n",
            "Iteration: 449; Percent complete: 22.4%; Average loss: 3.3685\n",
            "Iteration: 450; Percent complete: 22.5%; Average loss: 3.3972\n",
            "Iteration: 451; Percent complete: 22.6%; Average loss: 3.5153\n",
            "Iteration: 452; Percent complete: 22.6%; Average loss: 3.3548\n",
            "Iteration: 453; Percent complete: 22.7%; Average loss: 3.5049\n",
            "Iteration: 454; Percent complete: 22.7%; Average loss: 3.4571\n",
            "Iteration: 455; Percent complete: 22.8%; Average loss: 3.4045\n",
            "Iteration: 456; Percent complete: 22.8%; Average loss: 3.3967\n",
            "Iteration: 457; Percent complete: 22.9%; Average loss: 3.4012\n",
            "Iteration: 458; Percent complete: 22.9%; Average loss: 3.3669\n",
            "Iteration: 459; Percent complete: 22.9%; Average loss: 3.6534\n",
            "Iteration: 460; Percent complete: 23.0%; Average loss: 3.4312\n",
            "Iteration: 461; Percent complete: 23.1%; Average loss: 3.3827\n",
            "Iteration: 462; Percent complete: 23.1%; Average loss: 3.4159\n",
            "Iteration: 463; Percent complete: 23.2%; Average loss: 3.5401\n",
            "Iteration: 464; Percent complete: 23.2%; Average loss: 3.5984\n",
            "Iteration: 465; Percent complete: 23.2%; Average loss: 3.3379\n",
            "Iteration: 466; Percent complete: 23.3%; Average loss: 3.3761\n",
            "Iteration: 467; Percent complete: 23.4%; Average loss: 3.3778\n",
            "Iteration: 468; Percent complete: 23.4%; Average loss: 3.4847\n",
            "Iteration: 469; Percent complete: 23.4%; Average loss: 3.4487\n",
            "Iteration: 470; Percent complete: 23.5%; Average loss: 3.2952\n",
            "Iteration: 471; Percent complete: 23.5%; Average loss: 3.3872\n",
            "Iteration: 472; Percent complete: 23.6%; Average loss: 3.4720\n",
            "Iteration: 473; Percent complete: 23.6%; Average loss: 3.5093\n",
            "Iteration: 474; Percent complete: 23.7%; Average loss: 3.4114\n",
            "Iteration: 475; Percent complete: 23.8%; Average loss: 3.3967\n",
            "Iteration: 476; Percent complete: 23.8%; Average loss: 3.4043\n",
            "Iteration: 477; Percent complete: 23.8%; Average loss: 3.4284\n",
            "Iteration: 478; Percent complete: 23.9%; Average loss: 3.2236\n",
            "Iteration: 479; Percent complete: 23.9%; Average loss: 3.5346\n",
            "Iteration: 480; Percent complete: 24.0%; Average loss: 3.4636\n",
            "Iteration: 481; Percent complete: 24.1%; Average loss: 3.3863\n",
            "Iteration: 482; Percent complete: 24.1%; Average loss: 3.3750\n",
            "Iteration: 483; Percent complete: 24.1%; Average loss: 3.2372\n",
            "Iteration: 484; Percent complete: 24.2%; Average loss: 3.3642\n",
            "Iteration: 485; Percent complete: 24.2%; Average loss: 3.3616\n",
            "Iteration: 486; Percent complete: 24.3%; Average loss: 3.4876\n",
            "Iteration: 487; Percent complete: 24.3%; Average loss: 3.3380\n",
            "Iteration: 488; Percent complete: 24.4%; Average loss: 3.3481\n",
            "Iteration: 489; Percent complete: 24.4%; Average loss: 3.3576\n",
            "Iteration: 490; Percent complete: 24.5%; Average loss: 3.3485\n",
            "Iteration: 491; Percent complete: 24.6%; Average loss: 3.2664\n",
            "Iteration: 492; Percent complete: 24.6%; Average loss: 3.5575\n",
            "Iteration: 493; Percent complete: 24.6%; Average loss: 3.4757\n",
            "Iteration: 494; Percent complete: 24.7%; Average loss: 3.3053\n",
            "Iteration: 495; Percent complete: 24.8%; Average loss: 3.4763\n",
            "Iteration: 496; Percent complete: 24.8%; Average loss: 3.4489\n",
            "Iteration: 497; Percent complete: 24.9%; Average loss: 3.4487\n",
            "Iteration: 498; Percent complete: 24.9%; Average loss: 3.0985\n",
            "Iteration: 499; Percent complete: 24.9%; Average loss: 3.4043\n",
            "Iteration: 500; Percent complete: 25.0%; Average loss: 3.3774\n",
            "Iteration: 501; Percent complete: 25.1%; Average loss: 3.4206\n",
            "Iteration: 502; Percent complete: 25.1%; Average loss: 3.1719\n",
            "Iteration: 503; Percent complete: 25.1%; Average loss: 3.5101\n",
            "Iteration: 504; Percent complete: 25.2%; Average loss: 3.5608\n",
            "Iteration: 505; Percent complete: 25.2%; Average loss: 3.3683\n",
            "Iteration: 506; Percent complete: 25.3%; Average loss: 3.2635\n",
            "Iteration: 507; Percent complete: 25.4%; Average loss: 3.3094\n",
            "Iteration: 508; Percent complete: 25.4%; Average loss: 3.3341\n",
            "Iteration: 509; Percent complete: 25.4%; Average loss: 3.2951\n",
            "Iteration: 510; Percent complete: 25.5%; Average loss: 3.4053\n",
            "Iteration: 511; Percent complete: 25.6%; Average loss: 3.4292\n",
            "Iteration: 512; Percent complete: 25.6%; Average loss: 3.2893\n",
            "Iteration: 513; Percent complete: 25.7%; Average loss: 3.3870\n",
            "Iteration: 514; Percent complete: 25.7%; Average loss: 3.4520\n",
            "Iteration: 515; Percent complete: 25.8%; Average loss: 3.4128\n",
            "Iteration: 516; Percent complete: 25.8%; Average loss: 3.2804\n",
            "Iteration: 517; Percent complete: 25.9%; Average loss: 3.4998\n",
            "Iteration: 518; Percent complete: 25.9%; Average loss: 3.3386\n",
            "Iteration: 519; Percent complete: 25.9%; Average loss: 3.3582\n",
            "Iteration: 520; Percent complete: 26.0%; Average loss: 3.1416\n",
            "Iteration: 521; Percent complete: 26.1%; Average loss: 3.2104\n",
            "Iteration: 522; Percent complete: 26.1%; Average loss: 3.5376\n",
            "Iteration: 523; Percent complete: 26.2%; Average loss: 3.3024\n",
            "Iteration: 524; Percent complete: 26.2%; Average loss: 3.3026\n",
            "Iteration: 525; Percent complete: 26.2%; Average loss: 3.2810\n",
            "Iteration: 526; Percent complete: 26.3%; Average loss: 3.3253\n",
            "Iteration: 527; Percent complete: 26.4%; Average loss: 3.4239\n",
            "Iteration: 528; Percent complete: 26.4%; Average loss: 3.2171\n",
            "Iteration: 529; Percent complete: 26.5%; Average loss: 3.4434\n",
            "Iteration: 530; Percent complete: 26.5%; Average loss: 3.3726\n",
            "Iteration: 531; Percent complete: 26.6%; Average loss: 3.2184\n",
            "Iteration: 532; Percent complete: 26.6%; Average loss: 3.3010\n",
            "Iteration: 533; Percent complete: 26.7%; Average loss: 3.3540\n",
            "Iteration: 534; Percent complete: 26.7%; Average loss: 3.3584\n",
            "Iteration: 535; Percent complete: 26.8%; Average loss: 3.3142\n",
            "Iteration: 536; Percent complete: 26.8%; Average loss: 3.2149\n",
            "Iteration: 537; Percent complete: 26.9%; Average loss: 3.3912\n",
            "Iteration: 538; Percent complete: 26.9%; Average loss: 3.3140\n",
            "Iteration: 539; Percent complete: 27.0%; Average loss: 3.5273\n",
            "Iteration: 540; Percent complete: 27.0%; Average loss: 3.4270\n",
            "Iteration: 541; Percent complete: 27.1%; Average loss: 3.2470\n",
            "Iteration: 542; Percent complete: 27.1%; Average loss: 3.4128\n",
            "Iteration: 543; Percent complete: 27.2%; Average loss: 3.2822\n",
            "Iteration: 544; Percent complete: 27.2%; Average loss: 3.2226\n",
            "Iteration: 545; Percent complete: 27.3%; Average loss: 3.3344\n",
            "Iteration: 546; Percent complete: 27.3%; Average loss: 3.0663\n",
            "Iteration: 547; Percent complete: 27.4%; Average loss: 3.1785\n",
            "Iteration: 548; Percent complete: 27.4%; Average loss: 3.3122\n",
            "Iteration: 549; Percent complete: 27.5%; Average loss: 3.1777\n",
            "Iteration: 550; Percent complete: 27.5%; Average loss: 3.1533\n",
            "Iteration: 551; Percent complete: 27.6%; Average loss: 3.2557\n",
            "Iteration: 552; Percent complete: 27.6%; Average loss: 3.3282\n",
            "Iteration: 553; Percent complete: 27.7%; Average loss: 3.2566\n",
            "Iteration: 554; Percent complete: 27.7%; Average loss: 3.2239\n",
            "Iteration: 555; Percent complete: 27.8%; Average loss: 3.3735\n",
            "Iteration: 556; Percent complete: 27.8%; Average loss: 3.1552\n",
            "Iteration: 557; Percent complete: 27.9%; Average loss: 3.3815\n",
            "Iteration: 558; Percent complete: 27.9%; Average loss: 3.1475\n",
            "Iteration: 559; Percent complete: 28.0%; Average loss: 3.2926\n",
            "Iteration: 560; Percent complete: 28.0%; Average loss: 3.1351\n",
            "Iteration: 561; Percent complete: 28.1%; Average loss: 3.1652\n",
            "Iteration: 562; Percent complete: 28.1%; Average loss: 3.1808\n",
            "Iteration: 563; Percent complete: 28.1%; Average loss: 3.1766\n",
            "Iteration: 564; Percent complete: 28.2%; Average loss: 3.1712\n",
            "Iteration: 565; Percent complete: 28.2%; Average loss: 3.1299\n",
            "Iteration: 566; Percent complete: 28.3%; Average loss: 3.1188\n",
            "Iteration: 567; Percent complete: 28.3%; Average loss: 3.1940\n",
            "Iteration: 568; Percent complete: 28.4%; Average loss: 3.3553\n",
            "Iteration: 569; Percent complete: 28.4%; Average loss: 3.2979\n",
            "Iteration: 570; Percent complete: 28.5%; Average loss: 3.3639\n",
            "Iteration: 571; Percent complete: 28.5%; Average loss: 3.1841\n",
            "Iteration: 572; Percent complete: 28.6%; Average loss: 3.4733\n",
            "Iteration: 573; Percent complete: 28.6%; Average loss: 3.0929\n",
            "Iteration: 574; Percent complete: 28.7%; Average loss: 3.2802\n",
            "Iteration: 575; Percent complete: 28.7%; Average loss: 3.3203\n",
            "Iteration: 576; Percent complete: 28.8%; Average loss: 3.2059\n",
            "Iteration: 577; Percent complete: 28.8%; Average loss: 3.2182\n",
            "Iteration: 578; Percent complete: 28.9%; Average loss: 3.1688\n",
            "Iteration: 579; Percent complete: 28.9%; Average loss: 3.1178\n",
            "Iteration: 580; Percent complete: 29.0%; Average loss: 3.0601\n",
            "Iteration: 581; Percent complete: 29.0%; Average loss: 3.2916\n",
            "Iteration: 582; Percent complete: 29.1%; Average loss: 3.3865\n",
            "Iteration: 583; Percent complete: 29.1%; Average loss: 3.2883\n",
            "Iteration: 584; Percent complete: 29.2%; Average loss: 3.1743\n",
            "Iteration: 585; Percent complete: 29.2%; Average loss: 3.1780\n",
            "Iteration: 586; Percent complete: 29.3%; Average loss: 3.2156\n",
            "Iteration: 587; Percent complete: 29.3%; Average loss: 3.3799\n",
            "Iteration: 588; Percent complete: 29.4%; Average loss: 3.0844\n",
            "Iteration: 589; Percent complete: 29.4%; Average loss: 3.1386\n",
            "Iteration: 590; Percent complete: 29.5%; Average loss: 3.2561\n",
            "Iteration: 591; Percent complete: 29.5%; Average loss: 3.1522\n",
            "Iteration: 592; Percent complete: 29.6%; Average loss: 3.1491\n",
            "Iteration: 593; Percent complete: 29.6%; Average loss: 3.1801\n",
            "Iteration: 594; Percent complete: 29.7%; Average loss: 3.0834\n",
            "Iteration: 595; Percent complete: 29.8%; Average loss: 3.1583\n",
            "Iteration: 596; Percent complete: 29.8%; Average loss: 3.1356\n",
            "Iteration: 597; Percent complete: 29.8%; Average loss: 3.3402\n",
            "Iteration: 598; Percent complete: 29.9%; Average loss: 3.1788\n",
            "Iteration: 599; Percent complete: 29.9%; Average loss: 3.0420\n",
            "Iteration: 600; Percent complete: 30.0%; Average loss: 3.1119\n",
            "Iteration: 601; Percent complete: 30.0%; Average loss: 3.2582\n",
            "Iteration: 602; Percent complete: 30.1%; Average loss: 2.9478\n",
            "Iteration: 603; Percent complete: 30.1%; Average loss: 3.2734\n",
            "Iteration: 604; Percent complete: 30.2%; Average loss: 3.1251\n",
            "Iteration: 605; Percent complete: 30.2%; Average loss: 3.1689\n",
            "Iteration: 606; Percent complete: 30.3%; Average loss: 3.3328\n",
            "Iteration: 607; Percent complete: 30.3%; Average loss: 3.1119\n",
            "Iteration: 608; Percent complete: 30.4%; Average loss: 3.3199\n",
            "Iteration: 609; Percent complete: 30.4%; Average loss: 3.0143\n",
            "Iteration: 610; Percent complete: 30.5%; Average loss: 3.2758\n",
            "Iteration: 611; Percent complete: 30.6%; Average loss: 3.3077\n",
            "Iteration: 612; Percent complete: 30.6%; Average loss: 3.1172\n",
            "Iteration: 613; Percent complete: 30.6%; Average loss: 2.9079\n",
            "Iteration: 614; Percent complete: 30.7%; Average loss: 3.1289\n",
            "Iteration: 615; Percent complete: 30.8%; Average loss: 3.1513\n",
            "Iteration: 616; Percent complete: 30.8%; Average loss: 2.9963\n",
            "Iteration: 617; Percent complete: 30.9%; Average loss: 2.9430\n",
            "Iteration: 618; Percent complete: 30.9%; Average loss: 3.1039\n",
            "Iteration: 619; Percent complete: 30.9%; Average loss: 3.1019\n",
            "Iteration: 620; Percent complete: 31.0%; Average loss: 3.1786\n",
            "Iteration: 621; Percent complete: 31.1%; Average loss: 3.0763\n",
            "Iteration: 622; Percent complete: 31.1%; Average loss: 3.0800\n",
            "Iteration: 623; Percent complete: 31.1%; Average loss: 3.0592\n",
            "Iteration: 624; Percent complete: 31.2%; Average loss: 2.9445\n",
            "Iteration: 625; Percent complete: 31.2%; Average loss: 3.0914\n",
            "Iteration: 626; Percent complete: 31.3%; Average loss: 3.1956\n",
            "Iteration: 627; Percent complete: 31.4%; Average loss: 3.3081\n",
            "Iteration: 628; Percent complete: 31.4%; Average loss: 3.1571\n",
            "Iteration: 629; Percent complete: 31.4%; Average loss: 3.3975\n",
            "Iteration: 630; Percent complete: 31.5%; Average loss: 2.9339\n",
            "Iteration: 631; Percent complete: 31.6%; Average loss: 2.9645\n",
            "Iteration: 632; Percent complete: 31.6%; Average loss: 3.0996\n",
            "Iteration: 633; Percent complete: 31.6%; Average loss: 3.0763\n",
            "Iteration: 634; Percent complete: 31.7%; Average loss: 3.2893\n",
            "Iteration: 635; Percent complete: 31.8%; Average loss: 3.1784\n",
            "Iteration: 636; Percent complete: 31.8%; Average loss: 3.0240\n",
            "Iteration: 637; Percent complete: 31.9%; Average loss: 3.1697\n",
            "Iteration: 638; Percent complete: 31.9%; Average loss: 3.0482\n",
            "Iteration: 639; Percent complete: 31.9%; Average loss: 3.2818\n",
            "Iteration: 640; Percent complete: 32.0%; Average loss: 3.0443\n",
            "Iteration: 641; Percent complete: 32.0%; Average loss: 3.0374\n",
            "Iteration: 642; Percent complete: 32.1%; Average loss: 3.3149\n",
            "Iteration: 643; Percent complete: 32.1%; Average loss: 3.1094\n",
            "Iteration: 644; Percent complete: 32.2%; Average loss: 3.0909\n",
            "Iteration: 645; Percent complete: 32.2%; Average loss: 2.9298\n",
            "Iteration: 646; Percent complete: 32.3%; Average loss: 3.2189\n",
            "Iteration: 647; Percent complete: 32.4%; Average loss: 3.0724\n",
            "Iteration: 648; Percent complete: 32.4%; Average loss: 3.2141\n",
            "Iteration: 649; Percent complete: 32.5%; Average loss: 3.1322\n",
            "Iteration: 650; Percent complete: 32.5%; Average loss: 3.0766\n",
            "Iteration: 651; Percent complete: 32.6%; Average loss: 3.2215\n",
            "Iteration: 652; Percent complete: 32.6%; Average loss: 3.0982\n",
            "Iteration: 653; Percent complete: 32.6%; Average loss: 2.9515\n",
            "Iteration: 654; Percent complete: 32.7%; Average loss: 3.0642\n",
            "Iteration: 655; Percent complete: 32.8%; Average loss: 2.9576\n",
            "Iteration: 656; Percent complete: 32.8%; Average loss: 3.0363\n",
            "Iteration: 657; Percent complete: 32.9%; Average loss: 3.0154\n",
            "Iteration: 658; Percent complete: 32.9%; Average loss: 2.9872\n",
            "Iteration: 659; Percent complete: 33.0%; Average loss: 3.1621\n",
            "Iteration: 660; Percent complete: 33.0%; Average loss: 2.8993\n",
            "Iteration: 661; Percent complete: 33.1%; Average loss: 3.0989\n",
            "Iteration: 662; Percent complete: 33.1%; Average loss: 2.8851\n",
            "Iteration: 663; Percent complete: 33.1%; Average loss: 3.0417\n",
            "Iteration: 664; Percent complete: 33.2%; Average loss: 2.8239\n",
            "Iteration: 665; Percent complete: 33.2%; Average loss: 3.1829\n",
            "Iteration: 666; Percent complete: 33.3%; Average loss: 3.0476\n",
            "Iteration: 667; Percent complete: 33.4%; Average loss: 3.0400\n",
            "Iteration: 668; Percent complete: 33.4%; Average loss: 2.8299\n",
            "Iteration: 669; Percent complete: 33.5%; Average loss: 2.9177\n",
            "Iteration: 670; Percent complete: 33.5%; Average loss: 3.0404\n",
            "Iteration: 671; Percent complete: 33.6%; Average loss: 3.0496\n",
            "Iteration: 672; Percent complete: 33.6%; Average loss: 2.9820\n",
            "Iteration: 673; Percent complete: 33.7%; Average loss: 3.1592\n",
            "Iteration: 674; Percent complete: 33.7%; Average loss: 3.0056\n",
            "Iteration: 675; Percent complete: 33.8%; Average loss: 3.0520\n",
            "Iteration: 676; Percent complete: 33.8%; Average loss: 2.9604\n",
            "Iteration: 677; Percent complete: 33.9%; Average loss: 3.0729\n",
            "Iteration: 678; Percent complete: 33.9%; Average loss: 3.1749\n",
            "Iteration: 679; Percent complete: 34.0%; Average loss: 2.8884\n",
            "Iteration: 680; Percent complete: 34.0%; Average loss: 2.8999\n",
            "Iteration: 681; Percent complete: 34.1%; Average loss: 3.0843\n",
            "Iteration: 682; Percent complete: 34.1%; Average loss: 2.9188\n",
            "Iteration: 683; Percent complete: 34.2%; Average loss: 2.9834\n",
            "Iteration: 684; Percent complete: 34.2%; Average loss: 2.8423\n",
            "Iteration: 685; Percent complete: 34.2%; Average loss: 2.9880\n",
            "Iteration: 686; Percent complete: 34.3%; Average loss: 2.9778\n",
            "Iteration: 687; Percent complete: 34.4%; Average loss: 2.9913\n",
            "Iteration: 688; Percent complete: 34.4%; Average loss: 2.9164\n",
            "Iteration: 689; Percent complete: 34.4%; Average loss: 2.9528\n",
            "Iteration: 690; Percent complete: 34.5%; Average loss: 3.0953\n",
            "Iteration: 691; Percent complete: 34.5%; Average loss: 3.0982\n",
            "Iteration: 692; Percent complete: 34.6%; Average loss: 3.0715\n",
            "Iteration: 693; Percent complete: 34.6%; Average loss: 3.0245\n",
            "Iteration: 694; Percent complete: 34.7%; Average loss: 2.9986\n",
            "Iteration: 695; Percent complete: 34.8%; Average loss: 2.9470\n",
            "Iteration: 696; Percent complete: 34.8%; Average loss: 2.9977\n",
            "Iteration: 697; Percent complete: 34.8%; Average loss: 2.7849\n",
            "Iteration: 698; Percent complete: 34.9%; Average loss: 2.9920\n",
            "Iteration: 699; Percent complete: 34.9%; Average loss: 2.9414\n",
            "Iteration: 700; Percent complete: 35.0%; Average loss: 2.8951\n",
            "Iteration: 701; Percent complete: 35.0%; Average loss: 2.8043\n",
            "Iteration: 702; Percent complete: 35.1%; Average loss: 2.9377\n",
            "Iteration: 703; Percent complete: 35.1%; Average loss: 3.1280\n",
            "Iteration: 704; Percent complete: 35.2%; Average loss: 3.0953\n",
            "Iteration: 705; Percent complete: 35.2%; Average loss: 3.0107\n",
            "Iteration: 706; Percent complete: 35.3%; Average loss: 2.8583\n",
            "Iteration: 707; Percent complete: 35.4%; Average loss: 3.0364\n",
            "Iteration: 708; Percent complete: 35.4%; Average loss: 2.9041\n",
            "Iteration: 709; Percent complete: 35.4%; Average loss: 2.7628\n",
            "Iteration: 710; Percent complete: 35.5%; Average loss: 2.8533\n",
            "Iteration: 711; Percent complete: 35.5%; Average loss: 2.8169\n",
            "Iteration: 712; Percent complete: 35.6%; Average loss: 3.0023\n",
            "Iteration: 713; Percent complete: 35.6%; Average loss: 2.9590\n",
            "Iteration: 714; Percent complete: 35.7%; Average loss: 2.9530\n",
            "Iteration: 715; Percent complete: 35.8%; Average loss: 2.9403\n",
            "Iteration: 716; Percent complete: 35.8%; Average loss: 2.9659\n",
            "Iteration: 717; Percent complete: 35.9%; Average loss: 2.9148\n",
            "Iteration: 718; Percent complete: 35.9%; Average loss: 2.7409\n",
            "Iteration: 719; Percent complete: 35.9%; Average loss: 2.7919\n",
            "Iteration: 720; Percent complete: 36.0%; Average loss: 2.8068\n",
            "Iteration: 721; Percent complete: 36.0%; Average loss: 2.8751\n",
            "Iteration: 722; Percent complete: 36.1%; Average loss: 2.9075\n",
            "Iteration: 723; Percent complete: 36.1%; Average loss: 2.7945\n",
            "Iteration: 724; Percent complete: 36.2%; Average loss: 3.0232\n",
            "Iteration: 725; Percent complete: 36.2%; Average loss: 2.8585\n",
            "Iteration: 726; Percent complete: 36.3%; Average loss: 2.7862\n",
            "Iteration: 727; Percent complete: 36.4%; Average loss: 2.7635\n",
            "Iteration: 728; Percent complete: 36.4%; Average loss: 2.8266\n",
            "Iteration: 729; Percent complete: 36.4%; Average loss: 2.8416\n",
            "Iteration: 730; Percent complete: 36.5%; Average loss: 2.8131\n",
            "Iteration: 731; Percent complete: 36.5%; Average loss: 3.0288\n",
            "Iteration: 732; Percent complete: 36.6%; Average loss: 2.8899\n",
            "Iteration: 733; Percent complete: 36.6%; Average loss: 2.8316\n",
            "Iteration: 734; Percent complete: 36.7%; Average loss: 2.7746\n",
            "Iteration: 735; Percent complete: 36.8%; Average loss: 2.8343\n",
            "Iteration: 736; Percent complete: 36.8%; Average loss: 2.8558\n",
            "Iteration: 737; Percent complete: 36.9%; Average loss: 2.8958\n",
            "Iteration: 738; Percent complete: 36.9%; Average loss: 2.9420\n",
            "Iteration: 739; Percent complete: 37.0%; Average loss: 2.8681\n",
            "Iteration: 740; Percent complete: 37.0%; Average loss: 2.8810\n",
            "Iteration: 741; Percent complete: 37.0%; Average loss: 2.7727\n",
            "Iteration: 742; Percent complete: 37.1%; Average loss: 2.8514\n",
            "Iteration: 743; Percent complete: 37.1%; Average loss: 2.8353\n",
            "Iteration: 744; Percent complete: 37.2%; Average loss: 2.8657\n",
            "Iteration: 745; Percent complete: 37.2%; Average loss: 2.8795\n",
            "Iteration: 746; Percent complete: 37.3%; Average loss: 2.8143\n",
            "Iteration: 747; Percent complete: 37.4%; Average loss: 2.7440\n",
            "Iteration: 748; Percent complete: 37.4%; Average loss: 2.7533\n",
            "Iteration: 749; Percent complete: 37.5%; Average loss: 2.8878\n",
            "Iteration: 750; Percent complete: 37.5%; Average loss: 2.8360\n",
            "Iteration: 751; Percent complete: 37.5%; Average loss: 2.6903\n",
            "Iteration: 752; Percent complete: 37.6%; Average loss: 2.8689\n",
            "Iteration: 753; Percent complete: 37.6%; Average loss: 2.8840\n",
            "Iteration: 754; Percent complete: 37.7%; Average loss: 2.7699\n",
            "Iteration: 755; Percent complete: 37.8%; Average loss: 2.8162\n",
            "Iteration: 756; Percent complete: 37.8%; Average loss: 2.8924\n",
            "Iteration: 757; Percent complete: 37.9%; Average loss: 2.6295\n",
            "Iteration: 758; Percent complete: 37.9%; Average loss: 2.8048\n",
            "Iteration: 759; Percent complete: 38.0%; Average loss: 2.8016\n",
            "Iteration: 760; Percent complete: 38.0%; Average loss: 2.7573\n",
            "Iteration: 761; Percent complete: 38.0%; Average loss: 2.8857\n",
            "Iteration: 762; Percent complete: 38.1%; Average loss: 2.7201\n",
            "Iteration: 763; Percent complete: 38.1%; Average loss: 2.7705\n",
            "Iteration: 764; Percent complete: 38.2%; Average loss: 2.7713\n",
            "Iteration: 765; Percent complete: 38.2%; Average loss: 2.9340\n",
            "Iteration: 766; Percent complete: 38.3%; Average loss: 2.5932\n",
            "Iteration: 767; Percent complete: 38.4%; Average loss: 2.7281\n",
            "Iteration: 768; Percent complete: 38.4%; Average loss: 2.8059\n",
            "Iteration: 769; Percent complete: 38.5%; Average loss: 2.8641\n",
            "Iteration: 770; Percent complete: 38.5%; Average loss: 2.9155\n",
            "Iteration: 771; Percent complete: 38.6%; Average loss: 2.7666\n",
            "Iteration: 772; Percent complete: 38.6%; Average loss: 2.7954\n",
            "Iteration: 773; Percent complete: 38.6%; Average loss: 2.8632\n",
            "Iteration: 774; Percent complete: 38.7%; Average loss: 2.9666\n",
            "Iteration: 775; Percent complete: 38.8%; Average loss: 2.9237\n",
            "Iteration: 776; Percent complete: 38.8%; Average loss: 2.8207\n",
            "Iteration: 777; Percent complete: 38.9%; Average loss: 2.9787\n",
            "Iteration: 778; Percent complete: 38.9%; Average loss: 2.6735\n",
            "Iteration: 779; Percent complete: 39.0%; Average loss: 2.7890\n",
            "Iteration: 780; Percent complete: 39.0%; Average loss: 2.8863\n",
            "Iteration: 781; Percent complete: 39.1%; Average loss: 2.8052\n",
            "Iteration: 782; Percent complete: 39.1%; Average loss: 2.7243\n",
            "Iteration: 783; Percent complete: 39.1%; Average loss: 2.7805\n",
            "Iteration: 784; Percent complete: 39.2%; Average loss: 2.7979\n",
            "Iteration: 785; Percent complete: 39.2%; Average loss: 2.7654\n",
            "Iteration: 786; Percent complete: 39.3%; Average loss: 2.8213\n",
            "Iteration: 787; Percent complete: 39.4%; Average loss: 2.8365\n",
            "Iteration: 788; Percent complete: 39.4%; Average loss: 2.7972\n",
            "Iteration: 789; Percent complete: 39.5%; Average loss: 2.9271\n",
            "Iteration: 790; Percent complete: 39.5%; Average loss: 2.6921\n",
            "Iteration: 791; Percent complete: 39.6%; Average loss: 2.9018\n",
            "Iteration: 792; Percent complete: 39.6%; Average loss: 2.8737\n",
            "Iteration: 793; Percent complete: 39.6%; Average loss: 2.7384\n",
            "Iteration: 794; Percent complete: 39.7%; Average loss: 2.6166\n",
            "Iteration: 795; Percent complete: 39.8%; Average loss: 2.6765\n",
            "Iteration: 796; Percent complete: 39.8%; Average loss: 2.7683\n",
            "Iteration: 797; Percent complete: 39.9%; Average loss: 2.6926\n",
            "Iteration: 798; Percent complete: 39.9%; Average loss: 2.7262\n",
            "Iteration: 799; Percent complete: 40.0%; Average loss: 2.6735\n",
            "Iteration: 800; Percent complete: 40.0%; Average loss: 2.7638\n",
            "Iteration: 801; Percent complete: 40.1%; Average loss: 2.5023\n",
            "Iteration: 802; Percent complete: 40.1%; Average loss: 2.7710\n",
            "Iteration: 803; Percent complete: 40.2%; Average loss: 2.6840\n",
            "Iteration: 804; Percent complete: 40.2%; Average loss: 2.7520\n",
            "Iteration: 805; Percent complete: 40.2%; Average loss: 2.7825\n",
            "Iteration: 806; Percent complete: 40.3%; Average loss: 2.5222\n",
            "Iteration: 807; Percent complete: 40.4%; Average loss: 2.3842\n",
            "Iteration: 808; Percent complete: 40.4%; Average loss: 2.8021\n",
            "Iteration: 809; Percent complete: 40.5%; Average loss: 2.6185\n",
            "Iteration: 810; Percent complete: 40.5%; Average loss: 2.7814\n",
            "Iteration: 811; Percent complete: 40.6%; Average loss: 2.5863\n",
            "Iteration: 812; Percent complete: 40.6%; Average loss: 2.7616\n",
            "Iteration: 813; Percent complete: 40.6%; Average loss: 2.7079\n",
            "Iteration: 814; Percent complete: 40.7%; Average loss: 2.7497\n",
            "Iteration: 815; Percent complete: 40.8%; Average loss: 2.7477\n",
            "Iteration: 816; Percent complete: 40.8%; Average loss: 2.7950\n",
            "Iteration: 817; Percent complete: 40.8%; Average loss: 2.7757\n",
            "Iteration: 818; Percent complete: 40.9%; Average loss: 2.6984\n",
            "Iteration: 819; Percent complete: 40.9%; Average loss: 2.7146\n",
            "Iteration: 820; Percent complete: 41.0%; Average loss: 2.7498\n",
            "Iteration: 821; Percent complete: 41.0%; Average loss: 2.6242\n",
            "Iteration: 822; Percent complete: 41.1%; Average loss: 2.7185\n",
            "Iteration: 823; Percent complete: 41.1%; Average loss: 2.6545\n",
            "Iteration: 824; Percent complete: 41.2%; Average loss: 2.8749\n",
            "Iteration: 825; Percent complete: 41.2%; Average loss: 2.7178\n",
            "Iteration: 826; Percent complete: 41.3%; Average loss: 2.6854\n",
            "Iteration: 827; Percent complete: 41.3%; Average loss: 2.5971\n",
            "Iteration: 828; Percent complete: 41.4%; Average loss: 2.7804\n",
            "Iteration: 829; Percent complete: 41.4%; Average loss: 2.8873\n",
            "Iteration: 830; Percent complete: 41.5%; Average loss: 2.7712\n",
            "Iteration: 831; Percent complete: 41.5%; Average loss: 2.7280\n",
            "Iteration: 832; Percent complete: 41.6%; Average loss: 2.5383\n",
            "Iteration: 833; Percent complete: 41.6%; Average loss: 2.6800\n",
            "Iteration: 834; Percent complete: 41.7%; Average loss: 2.7100\n",
            "Iteration: 835; Percent complete: 41.8%; Average loss: 2.7536\n",
            "Iteration: 836; Percent complete: 41.8%; Average loss: 2.6805\n",
            "Iteration: 837; Percent complete: 41.9%; Average loss: 2.6199\n",
            "Iteration: 838; Percent complete: 41.9%; Average loss: 2.6316\n",
            "Iteration: 839; Percent complete: 41.9%; Average loss: 2.7644\n",
            "Iteration: 840; Percent complete: 42.0%; Average loss: 2.5643\n",
            "Iteration: 841; Percent complete: 42.0%; Average loss: 2.4786\n",
            "Iteration: 842; Percent complete: 42.1%; Average loss: 2.7840\n",
            "Iteration: 843; Percent complete: 42.1%; Average loss: 2.8251\n",
            "Iteration: 844; Percent complete: 42.2%; Average loss: 2.7922\n",
            "Iteration: 845; Percent complete: 42.2%; Average loss: 2.6492\n",
            "Iteration: 846; Percent complete: 42.3%; Average loss: 2.5511\n",
            "Iteration: 847; Percent complete: 42.4%; Average loss: 2.6611\n",
            "Iteration: 848; Percent complete: 42.4%; Average loss: 2.5899\n",
            "Iteration: 849; Percent complete: 42.4%; Average loss: 2.7470\n",
            "Iteration: 850; Percent complete: 42.5%; Average loss: 2.6036\n",
            "Iteration: 851; Percent complete: 42.5%; Average loss: 2.8833\n",
            "Iteration: 852; Percent complete: 42.6%; Average loss: 2.5976\n",
            "Iteration: 853; Percent complete: 42.6%; Average loss: 2.6016\n",
            "Iteration: 854; Percent complete: 42.7%; Average loss: 2.5759\n",
            "Iteration: 855; Percent complete: 42.8%; Average loss: 2.6841\n",
            "Iteration: 856; Percent complete: 42.8%; Average loss: 2.5196\n",
            "Iteration: 857; Percent complete: 42.9%; Average loss: 2.7267\n",
            "Iteration: 858; Percent complete: 42.9%; Average loss: 2.5038\n",
            "Iteration: 859; Percent complete: 43.0%; Average loss: 2.6363\n",
            "Iteration: 860; Percent complete: 43.0%; Average loss: 2.6740\n",
            "Iteration: 861; Percent complete: 43.0%; Average loss: 2.5538\n",
            "Iteration: 862; Percent complete: 43.1%; Average loss: 2.7394\n",
            "Iteration: 863; Percent complete: 43.1%; Average loss: 2.6695\n",
            "Iteration: 864; Percent complete: 43.2%; Average loss: 2.6831\n",
            "Iteration: 865; Percent complete: 43.2%; Average loss: 2.4922\n",
            "Iteration: 866; Percent complete: 43.3%; Average loss: 2.5572\n",
            "Iteration: 867; Percent complete: 43.4%; Average loss: 2.4893\n",
            "Iteration: 868; Percent complete: 43.4%; Average loss: 2.6966\n",
            "Iteration: 869; Percent complete: 43.5%; Average loss: 2.7299\n",
            "Iteration: 870; Percent complete: 43.5%; Average loss: 2.7017\n",
            "Iteration: 871; Percent complete: 43.5%; Average loss: 2.6557\n",
            "Iteration: 872; Percent complete: 43.6%; Average loss: 2.6768\n",
            "Iteration: 873; Percent complete: 43.6%; Average loss: 2.7204\n",
            "Iteration: 874; Percent complete: 43.7%; Average loss: 2.6350\n",
            "Iteration: 875; Percent complete: 43.8%; Average loss: 2.7066\n",
            "Iteration: 876; Percent complete: 43.8%; Average loss: 2.6317\n",
            "Iteration: 877; Percent complete: 43.9%; Average loss: 2.7522\n",
            "Iteration: 878; Percent complete: 43.9%; Average loss: 2.6223\n",
            "Iteration: 879; Percent complete: 44.0%; Average loss: 2.6356\n",
            "Iteration: 880; Percent complete: 44.0%; Average loss: 2.6588\n",
            "Iteration: 881; Percent complete: 44.0%; Average loss: 2.5678\n",
            "Iteration: 882; Percent complete: 44.1%; Average loss: 2.5968\n",
            "Iteration: 883; Percent complete: 44.1%; Average loss: 2.3576\n",
            "Iteration: 884; Percent complete: 44.2%; Average loss: 2.5396\n",
            "Iteration: 885; Percent complete: 44.2%; Average loss: 2.5409\n",
            "Iteration: 886; Percent complete: 44.3%; Average loss: 2.6192\n",
            "Iteration: 887; Percent complete: 44.4%; Average loss: 2.5133\n",
            "Iteration: 888; Percent complete: 44.4%; Average loss: 2.5657\n",
            "Iteration: 889; Percent complete: 44.5%; Average loss: 2.5857\n",
            "Iteration: 890; Percent complete: 44.5%; Average loss: 2.6310\n",
            "Iteration: 891; Percent complete: 44.5%; Average loss: 2.6398\n",
            "Iteration: 892; Percent complete: 44.6%; Average loss: 2.5289\n",
            "Iteration: 893; Percent complete: 44.6%; Average loss: 2.4957\n",
            "Iteration: 894; Percent complete: 44.7%; Average loss: 2.5551\n",
            "Iteration: 895; Percent complete: 44.8%; Average loss: 2.7434\n",
            "Iteration: 896; Percent complete: 44.8%; Average loss: 2.7561\n",
            "Iteration: 897; Percent complete: 44.9%; Average loss: 2.6107\n",
            "Iteration: 898; Percent complete: 44.9%; Average loss: 2.5793\n",
            "Iteration: 899; Percent complete: 45.0%; Average loss: 2.3938\n",
            "Iteration: 900; Percent complete: 45.0%; Average loss: 2.7914\n",
            "Iteration: 901; Percent complete: 45.1%; Average loss: 2.6508\n",
            "Iteration: 902; Percent complete: 45.1%; Average loss: 2.4992\n",
            "Iteration: 903; Percent complete: 45.1%; Average loss: 2.5362\n",
            "Iteration: 904; Percent complete: 45.2%; Average loss: 2.5255\n",
            "Iteration: 905; Percent complete: 45.2%; Average loss: 2.6374\n",
            "Iteration: 906; Percent complete: 45.3%; Average loss: 2.4776\n",
            "Iteration: 907; Percent complete: 45.4%; Average loss: 2.4948\n",
            "Iteration: 908; Percent complete: 45.4%; Average loss: 2.4066\n",
            "Iteration: 909; Percent complete: 45.5%; Average loss: 2.5131\n",
            "Iteration: 910; Percent complete: 45.5%; Average loss: 2.6742\n",
            "Iteration: 911; Percent complete: 45.6%; Average loss: 2.4883\n",
            "Iteration: 912; Percent complete: 45.6%; Average loss: 2.5799\n",
            "Iteration: 913; Percent complete: 45.6%; Average loss: 2.6385\n",
            "Iteration: 914; Percent complete: 45.7%; Average loss: 2.4975\n",
            "Iteration: 915; Percent complete: 45.8%; Average loss: 2.5318\n",
            "Iteration: 916; Percent complete: 45.8%; Average loss: 2.6156\n",
            "Iteration: 917; Percent complete: 45.9%; Average loss: 2.5145\n",
            "Iteration: 918; Percent complete: 45.9%; Average loss: 2.7113\n",
            "Iteration: 919; Percent complete: 46.0%; Average loss: 2.5073\n",
            "Iteration: 920; Percent complete: 46.0%; Average loss: 2.6238\n",
            "Iteration: 921; Percent complete: 46.1%; Average loss: 2.4579\n",
            "Iteration: 922; Percent complete: 46.1%; Average loss: 2.4593\n",
            "Iteration: 923; Percent complete: 46.2%; Average loss: 2.4421\n",
            "Iteration: 924; Percent complete: 46.2%; Average loss: 2.5982\n",
            "Iteration: 925; Percent complete: 46.2%; Average loss: 2.5244\n",
            "Iteration: 926; Percent complete: 46.3%; Average loss: 2.3685\n",
            "Iteration: 927; Percent complete: 46.4%; Average loss: 2.4794\n",
            "Iteration: 928; Percent complete: 46.4%; Average loss: 2.6347\n",
            "Iteration: 929; Percent complete: 46.5%; Average loss: 2.6180\n",
            "Iteration: 930; Percent complete: 46.5%; Average loss: 2.6367\n",
            "Iteration: 931; Percent complete: 46.6%; Average loss: 2.5491\n",
            "Iteration: 932; Percent complete: 46.6%; Average loss: 2.3951\n",
            "Iteration: 933; Percent complete: 46.7%; Average loss: 2.4033\n",
            "Iteration: 934; Percent complete: 46.7%; Average loss: 2.4311\n",
            "Iteration: 935; Percent complete: 46.8%; Average loss: 2.6517\n",
            "Iteration: 936; Percent complete: 46.8%; Average loss: 2.5509\n",
            "Iteration: 937; Percent complete: 46.9%; Average loss: 2.5281\n",
            "Iteration: 938; Percent complete: 46.9%; Average loss: 2.5544\n",
            "Iteration: 939; Percent complete: 46.9%; Average loss: 2.6321\n",
            "Iteration: 940; Percent complete: 47.0%; Average loss: 2.5427\n",
            "Iteration: 941; Percent complete: 47.0%; Average loss: 2.4533\n",
            "Iteration: 942; Percent complete: 47.1%; Average loss: 2.4222\n",
            "Iteration: 943; Percent complete: 47.1%; Average loss: 2.4623\n",
            "Iteration: 944; Percent complete: 47.2%; Average loss: 2.5155\n",
            "Iteration: 945; Percent complete: 47.2%; Average loss: 2.5155\n",
            "Iteration: 946; Percent complete: 47.3%; Average loss: 2.4939\n",
            "Iteration: 947; Percent complete: 47.3%; Average loss: 2.4390\n",
            "Iteration: 948; Percent complete: 47.4%; Average loss: 2.4690\n",
            "Iteration: 949; Percent complete: 47.4%; Average loss: 2.3263\n",
            "Iteration: 950; Percent complete: 47.5%; Average loss: 2.4893\n",
            "Iteration: 951; Percent complete: 47.5%; Average loss: 2.5599\n",
            "Iteration: 952; Percent complete: 47.6%; Average loss: 2.5610\n",
            "Iteration: 953; Percent complete: 47.6%; Average loss: 2.6903\n",
            "Iteration: 954; Percent complete: 47.7%; Average loss: 2.5443\n",
            "Iteration: 955; Percent complete: 47.8%; Average loss: 2.4376\n",
            "Iteration: 956; Percent complete: 47.8%; Average loss: 2.4514\n",
            "Iteration: 957; Percent complete: 47.9%; Average loss: 2.4592\n",
            "Iteration: 958; Percent complete: 47.9%; Average loss: 2.5344\n",
            "Iteration: 959; Percent complete: 47.9%; Average loss: 2.4675\n",
            "Iteration: 960; Percent complete: 48.0%; Average loss: 2.6063\n",
            "Iteration: 961; Percent complete: 48.0%; Average loss: 2.4896\n",
            "Iteration: 962; Percent complete: 48.1%; Average loss: 2.4262\n",
            "Iteration: 963; Percent complete: 48.1%; Average loss: 2.3666\n",
            "Iteration: 964; Percent complete: 48.2%; Average loss: 2.4473\n",
            "Iteration: 965; Percent complete: 48.2%; Average loss: 2.5675\n",
            "Iteration: 966; Percent complete: 48.3%; Average loss: 2.7063\n",
            "Iteration: 967; Percent complete: 48.4%; Average loss: 2.3808\n",
            "Iteration: 968; Percent complete: 48.4%; Average loss: 2.5344\n",
            "Iteration: 969; Percent complete: 48.4%; Average loss: 2.5203\n",
            "Iteration: 970; Percent complete: 48.5%; Average loss: 2.4528\n",
            "Iteration: 971; Percent complete: 48.5%; Average loss: 2.2642\n",
            "Iteration: 972; Percent complete: 48.6%; Average loss: 2.5386\n",
            "Iteration: 973; Percent complete: 48.6%; Average loss: 2.3494\n",
            "Iteration: 974; Percent complete: 48.7%; Average loss: 2.4094\n",
            "Iteration: 975; Percent complete: 48.8%; Average loss: 2.4559\n",
            "Iteration: 976; Percent complete: 48.8%; Average loss: 2.5011\n",
            "Iteration: 977; Percent complete: 48.9%; Average loss: 2.4283\n",
            "Iteration: 978; Percent complete: 48.9%; Average loss: 2.2968\n",
            "Iteration: 979; Percent complete: 48.9%; Average loss: 2.4151\n",
            "Iteration: 980; Percent complete: 49.0%; Average loss: 2.5562\n",
            "Iteration: 981; Percent complete: 49.0%; Average loss: 2.3775\n",
            "Iteration: 982; Percent complete: 49.1%; Average loss: 2.4919\n",
            "Iteration: 983; Percent complete: 49.1%; Average loss: 2.4119\n",
            "Iteration: 984; Percent complete: 49.2%; Average loss: 2.4684\n",
            "Iteration: 985; Percent complete: 49.2%; Average loss: 2.4121\n",
            "Iteration: 986; Percent complete: 49.3%; Average loss: 2.3603\n",
            "Iteration: 987; Percent complete: 49.4%; Average loss: 2.4404\n",
            "Iteration: 988; Percent complete: 49.4%; Average loss: 2.4641\n",
            "Iteration: 989; Percent complete: 49.5%; Average loss: 2.4935\n",
            "Iteration: 990; Percent complete: 49.5%; Average loss: 2.4458\n",
            "Iteration: 991; Percent complete: 49.5%; Average loss: 2.3503\n",
            "Iteration: 992; Percent complete: 49.6%; Average loss: 2.3550\n",
            "Iteration: 993; Percent complete: 49.6%; Average loss: 2.5229\n",
            "Iteration: 994; Percent complete: 49.7%; Average loss: 2.6095\n",
            "Iteration: 995; Percent complete: 49.8%; Average loss: 2.2592\n",
            "Iteration: 996; Percent complete: 49.8%; Average loss: 2.3438\n",
            "Iteration: 997; Percent complete: 49.9%; Average loss: 2.2834\n",
            "Iteration: 998; Percent complete: 49.9%; Average loss: 2.4462\n",
            "Iteration: 999; Percent complete: 50.0%; Average loss: 2.5979\n",
            "Iteration: 1000; Percent complete: 50.0%; Average loss: 2.4682\n",
            "Iteration: 1001; Percent complete: 50.0%; Average loss: 2.5039\n",
            "Iteration: 1002; Percent complete: 50.1%; Average loss: 2.2992\n",
            "Iteration: 1003; Percent complete: 50.1%; Average loss: 2.2385\n",
            "Iteration: 1004; Percent complete: 50.2%; Average loss: 2.4372\n",
            "Iteration: 1005; Percent complete: 50.2%; Average loss: 2.3892\n",
            "Iteration: 1006; Percent complete: 50.3%; Average loss: 2.3314\n",
            "Iteration: 1007; Percent complete: 50.3%; Average loss: 2.3751\n",
            "Iteration: 1008; Percent complete: 50.4%; Average loss: 2.3265\n",
            "Iteration: 1009; Percent complete: 50.4%; Average loss: 2.4565\n",
            "Iteration: 1010; Percent complete: 50.5%; Average loss: 2.4330\n",
            "Iteration: 1011; Percent complete: 50.5%; Average loss: 2.4198\n",
            "Iteration: 1012; Percent complete: 50.6%; Average loss: 2.3385\n",
            "Iteration: 1013; Percent complete: 50.6%; Average loss: 2.4869\n",
            "Iteration: 1014; Percent complete: 50.7%; Average loss: 2.2837\n",
            "Iteration: 1015; Percent complete: 50.7%; Average loss: 2.3073\n",
            "Iteration: 1016; Percent complete: 50.8%; Average loss: 2.3002\n",
            "Iteration: 1017; Percent complete: 50.8%; Average loss: 2.3353\n",
            "Iteration: 1018; Percent complete: 50.9%; Average loss: 2.4160\n",
            "Iteration: 1019; Percent complete: 50.9%; Average loss: 2.3982\n",
            "Iteration: 1020; Percent complete: 51.0%; Average loss: 2.3297\n",
            "Iteration: 1021; Percent complete: 51.0%; Average loss: 2.3473\n",
            "Iteration: 1022; Percent complete: 51.1%; Average loss: 2.5112\n",
            "Iteration: 1023; Percent complete: 51.1%; Average loss: 2.3159\n",
            "Iteration: 1024; Percent complete: 51.2%; Average loss: 2.2234\n",
            "Iteration: 1025; Percent complete: 51.2%; Average loss: 2.3856\n",
            "Iteration: 1026; Percent complete: 51.3%; Average loss: 2.2736\n",
            "Iteration: 1027; Percent complete: 51.3%; Average loss: 2.2639\n",
            "Iteration: 1028; Percent complete: 51.4%; Average loss: 2.1997\n",
            "Iteration: 1029; Percent complete: 51.4%; Average loss: 2.4719\n",
            "Iteration: 1030; Percent complete: 51.5%; Average loss: 2.3585\n",
            "Iteration: 1031; Percent complete: 51.5%; Average loss: 2.3025\n",
            "Iteration: 1032; Percent complete: 51.6%; Average loss: 2.2636\n",
            "Iteration: 1033; Percent complete: 51.6%; Average loss: 2.4366\n",
            "Iteration: 1034; Percent complete: 51.7%; Average loss: 2.3457\n",
            "Iteration: 1035; Percent complete: 51.7%; Average loss: 2.2959\n",
            "Iteration: 1036; Percent complete: 51.8%; Average loss: 2.2548\n",
            "Iteration: 1037; Percent complete: 51.8%; Average loss: 2.4497\n",
            "Iteration: 1038; Percent complete: 51.9%; Average loss: 2.2513\n",
            "Iteration: 1039; Percent complete: 51.9%; Average loss: 2.4070\n",
            "Iteration: 1040; Percent complete: 52.0%; Average loss: 2.4318\n",
            "Iteration: 1041; Percent complete: 52.0%; Average loss: 2.2374\n",
            "Iteration: 1042; Percent complete: 52.1%; Average loss: 2.3340\n",
            "Iteration: 1043; Percent complete: 52.1%; Average loss: 2.2896\n",
            "Iteration: 1044; Percent complete: 52.2%; Average loss: 2.3274\n",
            "Iteration: 1045; Percent complete: 52.2%; Average loss: 2.3648\n",
            "Iteration: 1046; Percent complete: 52.3%; Average loss: 2.4030\n",
            "Iteration: 1047; Percent complete: 52.3%; Average loss: 2.1697\n",
            "Iteration: 1048; Percent complete: 52.4%; Average loss: 2.4530\n",
            "Iteration: 1049; Percent complete: 52.4%; Average loss: 2.2326\n",
            "Iteration: 1050; Percent complete: 52.5%; Average loss: 2.1792\n",
            "Iteration: 1051; Percent complete: 52.5%; Average loss: 2.3783\n",
            "Iteration: 1052; Percent complete: 52.6%; Average loss: 2.4284\n",
            "Iteration: 1053; Percent complete: 52.6%; Average loss: 2.3111\n",
            "Iteration: 1054; Percent complete: 52.7%; Average loss: 2.3303\n",
            "Iteration: 1055; Percent complete: 52.8%; Average loss: 2.3785\n",
            "Iteration: 1056; Percent complete: 52.8%; Average loss: 2.2989\n",
            "Iteration: 1057; Percent complete: 52.8%; Average loss: 2.0631\n",
            "Iteration: 1058; Percent complete: 52.9%; Average loss: 2.3998\n",
            "Iteration: 1059; Percent complete: 52.9%; Average loss: 2.2012\n",
            "Iteration: 1060; Percent complete: 53.0%; Average loss: 2.3286\n",
            "Iteration: 1061; Percent complete: 53.0%; Average loss: 2.3979\n",
            "Iteration: 1062; Percent complete: 53.1%; Average loss: 2.1430\n",
            "Iteration: 1063; Percent complete: 53.1%; Average loss: 2.4112\n",
            "Iteration: 1064; Percent complete: 53.2%; Average loss: 2.2352\n",
            "Iteration: 1065; Percent complete: 53.2%; Average loss: 2.3301\n",
            "Iteration: 1066; Percent complete: 53.3%; Average loss: 2.2996\n",
            "Iteration: 1067; Percent complete: 53.3%; Average loss: 2.3652\n",
            "Iteration: 1068; Percent complete: 53.4%; Average loss: 2.3870\n",
            "Iteration: 1069; Percent complete: 53.4%; Average loss: 2.2065\n",
            "Iteration: 1070; Percent complete: 53.5%; Average loss: 2.3289\n",
            "Iteration: 1071; Percent complete: 53.5%; Average loss: 2.3476\n",
            "Iteration: 1072; Percent complete: 53.6%; Average loss: 2.1359\n",
            "Iteration: 1073; Percent complete: 53.6%; Average loss: 2.2687\n",
            "Iteration: 1074; Percent complete: 53.7%; Average loss: 2.3196\n",
            "Iteration: 1075; Percent complete: 53.8%; Average loss: 2.3292\n",
            "Iteration: 1076; Percent complete: 53.8%; Average loss: 2.2010\n",
            "Iteration: 1077; Percent complete: 53.8%; Average loss: 2.2589\n",
            "Iteration: 1078; Percent complete: 53.9%; Average loss: 2.4648\n",
            "Iteration: 1079; Percent complete: 53.9%; Average loss: 2.2945\n",
            "Iteration: 1080; Percent complete: 54.0%; Average loss: 2.2333\n",
            "Iteration: 1081; Percent complete: 54.0%; Average loss: 2.4249\n",
            "Iteration: 1082; Percent complete: 54.1%; Average loss: 2.2877\n",
            "Iteration: 1083; Percent complete: 54.1%; Average loss: 2.2559\n",
            "Iteration: 1084; Percent complete: 54.2%; Average loss: 2.1991\n",
            "Iteration: 1085; Percent complete: 54.2%; Average loss: 2.2466\n",
            "Iteration: 1086; Percent complete: 54.3%; Average loss: 2.2423\n",
            "Iteration: 1087; Percent complete: 54.4%; Average loss: 2.1347\n",
            "Iteration: 1088; Percent complete: 54.4%; Average loss: 2.3326\n",
            "Iteration: 1089; Percent complete: 54.4%; Average loss: 2.3351\n",
            "Iteration: 1090; Percent complete: 54.5%; Average loss: 2.1630\n",
            "Iteration: 1091; Percent complete: 54.5%; Average loss: 2.2154\n",
            "Iteration: 1092; Percent complete: 54.6%; Average loss: 2.2694\n",
            "Iteration: 1093; Percent complete: 54.6%; Average loss: 2.1469\n",
            "Iteration: 1094; Percent complete: 54.7%; Average loss: 2.2398\n",
            "Iteration: 1095; Percent complete: 54.8%; Average loss: 2.2598\n",
            "Iteration: 1096; Percent complete: 54.8%; Average loss: 2.2890\n",
            "Iteration: 1097; Percent complete: 54.9%; Average loss: 2.1125\n",
            "Iteration: 1098; Percent complete: 54.9%; Average loss: 2.1736\n",
            "Iteration: 1099; Percent complete: 54.9%; Average loss: 2.1546\n",
            "Iteration: 1100; Percent complete: 55.0%; Average loss: 2.2027\n",
            "Iteration: 1101; Percent complete: 55.0%; Average loss: 2.1971\n",
            "Iteration: 1102; Percent complete: 55.1%; Average loss: 2.2001\n",
            "Iteration: 1103; Percent complete: 55.1%; Average loss: 2.1630\n",
            "Iteration: 1104; Percent complete: 55.2%; Average loss: 2.3262\n",
            "Iteration: 1105; Percent complete: 55.2%; Average loss: 2.4061\n",
            "Iteration: 1106; Percent complete: 55.3%; Average loss: 2.0528\n",
            "Iteration: 1107; Percent complete: 55.4%; Average loss: 2.1983\n",
            "Iteration: 1108; Percent complete: 55.4%; Average loss: 2.2374\n",
            "Iteration: 1109; Percent complete: 55.5%; Average loss: 2.2071\n",
            "Iteration: 1110; Percent complete: 55.5%; Average loss: 2.4212\n",
            "Iteration: 1111; Percent complete: 55.5%; Average loss: 2.1645\n",
            "Iteration: 1112; Percent complete: 55.6%; Average loss: 1.9962\n",
            "Iteration: 1113; Percent complete: 55.6%; Average loss: 2.2946\n",
            "Iteration: 1114; Percent complete: 55.7%; Average loss: 2.1860\n",
            "Iteration: 1115; Percent complete: 55.8%; Average loss: 2.1243\n",
            "Iteration: 1116; Percent complete: 55.8%; Average loss: 2.1877\n",
            "Iteration: 1117; Percent complete: 55.9%; Average loss: 2.0609\n",
            "Iteration: 1118; Percent complete: 55.9%; Average loss: 2.1933\n",
            "Iteration: 1119; Percent complete: 56.0%; Average loss: 2.2621\n",
            "Iteration: 1120; Percent complete: 56.0%; Average loss: 2.3103\n",
            "Iteration: 1121; Percent complete: 56.0%; Average loss: 2.0856\n",
            "Iteration: 1122; Percent complete: 56.1%; Average loss: 2.0959\n",
            "Iteration: 1123; Percent complete: 56.1%; Average loss: 2.1516\n",
            "Iteration: 1124; Percent complete: 56.2%; Average loss: 2.1533\n",
            "Iteration: 1125; Percent complete: 56.2%; Average loss: 2.1451\n",
            "Iteration: 1126; Percent complete: 56.3%; Average loss: 2.2442\n",
            "Iteration: 1127; Percent complete: 56.4%; Average loss: 2.2291\n",
            "Iteration: 1128; Percent complete: 56.4%; Average loss: 2.0911\n",
            "Iteration: 1129; Percent complete: 56.5%; Average loss: 2.1495\n",
            "Iteration: 1130; Percent complete: 56.5%; Average loss: 2.1416\n",
            "Iteration: 1131; Percent complete: 56.5%; Average loss: 2.1295\n",
            "Iteration: 1132; Percent complete: 56.6%; Average loss: 1.9586\n",
            "Iteration: 1133; Percent complete: 56.6%; Average loss: 2.2053\n",
            "Iteration: 1134; Percent complete: 56.7%; Average loss: 2.0899\n",
            "Iteration: 1135; Percent complete: 56.8%; Average loss: 2.2637\n",
            "Iteration: 1136; Percent complete: 56.8%; Average loss: 2.0595\n",
            "Iteration: 1137; Percent complete: 56.9%; Average loss: 2.1509\n",
            "Iteration: 1138; Percent complete: 56.9%; Average loss: 2.0259\n",
            "Iteration: 1139; Percent complete: 57.0%; Average loss: 2.1122\n",
            "Iteration: 1140; Percent complete: 57.0%; Average loss: 2.0889\n",
            "Iteration: 1141; Percent complete: 57.0%; Average loss: 2.0974\n",
            "Iteration: 1142; Percent complete: 57.1%; Average loss: 2.0424\n",
            "Iteration: 1143; Percent complete: 57.1%; Average loss: 2.1020\n",
            "Iteration: 1144; Percent complete: 57.2%; Average loss: 2.1103\n",
            "Iteration: 1145; Percent complete: 57.2%; Average loss: 2.1611\n",
            "Iteration: 1146; Percent complete: 57.3%; Average loss: 2.1330\n",
            "Iteration: 1147; Percent complete: 57.4%; Average loss: 2.0841\n",
            "Iteration: 1148; Percent complete: 57.4%; Average loss: 2.0022\n",
            "Iteration: 1149; Percent complete: 57.5%; Average loss: 2.1101\n",
            "Iteration: 1150; Percent complete: 57.5%; Average loss: 2.0344\n",
            "Iteration: 1151; Percent complete: 57.6%; Average loss: 1.9753\n",
            "Iteration: 1152; Percent complete: 57.6%; Average loss: 2.0577\n",
            "Iteration: 1153; Percent complete: 57.6%; Average loss: 2.1493\n",
            "Iteration: 1154; Percent complete: 57.7%; Average loss: 2.1034\n",
            "Iteration: 1155; Percent complete: 57.8%; Average loss: 2.2364\n",
            "Iteration: 1156; Percent complete: 57.8%; Average loss: 2.2132\n",
            "Iteration: 1157; Percent complete: 57.9%; Average loss: 2.0799\n",
            "Iteration: 1158; Percent complete: 57.9%; Average loss: 2.0337\n",
            "Iteration: 1159; Percent complete: 58.0%; Average loss: 2.1075\n",
            "Iteration: 1160; Percent complete: 58.0%; Average loss: 2.1691\n",
            "Iteration: 1161; Percent complete: 58.1%; Average loss: 2.1138\n",
            "Iteration: 1162; Percent complete: 58.1%; Average loss: 1.9777\n",
            "Iteration: 1163; Percent complete: 58.1%; Average loss: 2.0022\n",
            "Iteration: 1164; Percent complete: 58.2%; Average loss: 2.1109\n",
            "Iteration: 1165; Percent complete: 58.2%; Average loss: 2.1878\n",
            "Iteration: 1166; Percent complete: 58.3%; Average loss: 2.1232\n",
            "Iteration: 1167; Percent complete: 58.4%; Average loss: 2.1529\n",
            "Iteration: 1168; Percent complete: 58.4%; Average loss: 2.0981\n",
            "Iteration: 1169; Percent complete: 58.5%; Average loss: 2.0614\n",
            "Iteration: 1170; Percent complete: 58.5%; Average loss: 2.1513\n",
            "Iteration: 1171; Percent complete: 58.6%; Average loss: 2.0644\n",
            "Iteration: 1172; Percent complete: 58.6%; Average loss: 2.0363\n",
            "Iteration: 1173; Percent complete: 58.7%; Average loss: 2.0388\n",
            "Iteration: 1174; Percent complete: 58.7%; Average loss: 2.0016\n",
            "Iteration: 1175; Percent complete: 58.8%; Average loss: 2.2910\n",
            "Iteration: 1176; Percent complete: 58.8%; Average loss: 2.0486\n",
            "Iteration: 1177; Percent complete: 58.9%; Average loss: 1.9972\n",
            "Iteration: 1178; Percent complete: 58.9%; Average loss: 2.1291\n",
            "Iteration: 1179; Percent complete: 59.0%; Average loss: 2.1029\n",
            "Iteration: 1180; Percent complete: 59.0%; Average loss: 2.1121\n",
            "Iteration: 1181; Percent complete: 59.1%; Average loss: 1.9045\n",
            "Iteration: 1182; Percent complete: 59.1%; Average loss: 1.9631\n",
            "Iteration: 1183; Percent complete: 59.2%; Average loss: 2.0259\n",
            "Iteration: 1184; Percent complete: 59.2%; Average loss: 1.9998\n",
            "Iteration: 1185; Percent complete: 59.2%; Average loss: 1.9752\n",
            "Iteration: 1186; Percent complete: 59.3%; Average loss: 2.1335\n",
            "Iteration: 1187; Percent complete: 59.4%; Average loss: 2.0387\n",
            "Iteration: 1188; Percent complete: 59.4%; Average loss: 2.1050\n",
            "Iteration: 1189; Percent complete: 59.5%; Average loss: 1.9936\n",
            "Iteration: 1190; Percent complete: 59.5%; Average loss: 2.0194\n",
            "Iteration: 1191; Percent complete: 59.6%; Average loss: 2.1144\n",
            "Iteration: 1192; Percent complete: 59.6%; Average loss: 1.9069\n",
            "Iteration: 1193; Percent complete: 59.7%; Average loss: 1.9722\n",
            "Iteration: 1194; Percent complete: 59.7%; Average loss: 1.9892\n",
            "Iteration: 1195; Percent complete: 59.8%; Average loss: 1.9732\n",
            "Iteration: 1196; Percent complete: 59.8%; Average loss: 2.1305\n",
            "Iteration: 1197; Percent complete: 59.9%; Average loss: 2.1856\n",
            "Iteration: 1198; Percent complete: 59.9%; Average loss: 1.9192\n",
            "Iteration: 1199; Percent complete: 60.0%; Average loss: 2.0402\n",
            "Iteration: 1200; Percent complete: 60.0%; Average loss: 1.9994\n",
            "Iteration: 1201; Percent complete: 60.1%; Average loss: 2.1378\n",
            "Iteration: 1202; Percent complete: 60.1%; Average loss: 2.0925\n",
            "Iteration: 1203; Percent complete: 60.2%; Average loss: 2.0345\n",
            "Iteration: 1204; Percent complete: 60.2%; Average loss: 1.9778\n",
            "Iteration: 1205; Percent complete: 60.2%; Average loss: 2.0523\n",
            "Iteration: 1206; Percent complete: 60.3%; Average loss: 1.9006\n",
            "Iteration: 1207; Percent complete: 60.4%; Average loss: 1.9703\n",
            "Iteration: 1208; Percent complete: 60.4%; Average loss: 2.0758\n",
            "Iteration: 1209; Percent complete: 60.5%; Average loss: 1.9407\n",
            "Iteration: 1210; Percent complete: 60.5%; Average loss: 2.1276\n",
            "Iteration: 1211; Percent complete: 60.6%; Average loss: 1.9854\n",
            "Iteration: 1212; Percent complete: 60.6%; Average loss: 2.0177\n",
            "Iteration: 1213; Percent complete: 60.7%; Average loss: 1.9473\n",
            "Iteration: 1214; Percent complete: 60.7%; Average loss: 1.9762\n",
            "Iteration: 1215; Percent complete: 60.8%; Average loss: 2.0092\n",
            "Iteration: 1216; Percent complete: 60.8%; Average loss: 1.9876\n",
            "Iteration: 1217; Percent complete: 60.9%; Average loss: 1.9816\n",
            "Iteration: 1218; Percent complete: 60.9%; Average loss: 1.8494\n",
            "Iteration: 1219; Percent complete: 61.0%; Average loss: 2.1379\n",
            "Iteration: 1220; Percent complete: 61.0%; Average loss: 2.0010\n",
            "Iteration: 1221; Percent complete: 61.1%; Average loss: 1.9080\n",
            "Iteration: 1222; Percent complete: 61.1%; Average loss: 2.0234\n",
            "Iteration: 1223; Percent complete: 61.2%; Average loss: 2.0524\n",
            "Iteration: 1224; Percent complete: 61.2%; Average loss: 2.0451\n",
            "Iteration: 1225; Percent complete: 61.3%; Average loss: 2.0134\n",
            "Iteration: 1226; Percent complete: 61.3%; Average loss: 1.9268\n",
            "Iteration: 1227; Percent complete: 61.4%; Average loss: 1.9749\n",
            "Iteration: 1228; Percent complete: 61.4%; Average loss: 2.0358\n",
            "Iteration: 1229; Percent complete: 61.5%; Average loss: 1.9213\n",
            "Iteration: 1230; Percent complete: 61.5%; Average loss: 2.0728\n",
            "Iteration: 1231; Percent complete: 61.6%; Average loss: 1.8957\n",
            "Iteration: 1232; Percent complete: 61.6%; Average loss: 2.0535\n",
            "Iteration: 1233; Percent complete: 61.7%; Average loss: 1.9308\n",
            "Iteration: 1234; Percent complete: 61.7%; Average loss: 1.9671\n",
            "Iteration: 1235; Percent complete: 61.8%; Average loss: 1.9549\n",
            "Iteration: 1236; Percent complete: 61.8%; Average loss: 1.9107\n",
            "Iteration: 1237; Percent complete: 61.9%; Average loss: 2.1386\n",
            "Iteration: 1238; Percent complete: 61.9%; Average loss: 1.9142\n",
            "Iteration: 1239; Percent complete: 62.0%; Average loss: 1.9332\n",
            "Iteration: 1240; Percent complete: 62.0%; Average loss: 1.9929\n",
            "Iteration: 1241; Percent complete: 62.1%; Average loss: 2.0581\n",
            "Iteration: 1242; Percent complete: 62.1%; Average loss: 2.0640\n",
            "Iteration: 1243; Percent complete: 62.2%; Average loss: 1.8843\n",
            "Iteration: 1244; Percent complete: 62.2%; Average loss: 1.8751\n",
            "Iteration: 1245; Percent complete: 62.3%; Average loss: 1.8974\n",
            "Iteration: 1246; Percent complete: 62.3%; Average loss: 1.9243\n",
            "Iteration: 1247; Percent complete: 62.4%; Average loss: 1.9313\n",
            "Iteration: 1248; Percent complete: 62.4%; Average loss: 1.9402\n",
            "Iteration: 1249; Percent complete: 62.5%; Average loss: 1.9224\n",
            "Iteration: 1250; Percent complete: 62.5%; Average loss: 1.8087\n",
            "Iteration: 1251; Percent complete: 62.5%; Average loss: 1.9920\n",
            "Iteration: 1252; Percent complete: 62.6%; Average loss: 1.8101\n",
            "Iteration: 1253; Percent complete: 62.6%; Average loss: 1.9613\n",
            "Iteration: 1254; Percent complete: 62.7%; Average loss: 1.9067\n",
            "Iteration: 1255; Percent complete: 62.7%; Average loss: 2.0331\n",
            "Iteration: 1256; Percent complete: 62.8%; Average loss: 1.9469\n",
            "Iteration: 1257; Percent complete: 62.8%; Average loss: 1.8344\n",
            "Iteration: 1258; Percent complete: 62.9%; Average loss: 1.9838\n",
            "Iteration: 1259; Percent complete: 62.9%; Average loss: 1.9314\n",
            "Iteration: 1260; Percent complete: 63.0%; Average loss: 2.0052\n",
            "Iteration: 1261; Percent complete: 63.0%; Average loss: 1.9406\n",
            "Iteration: 1262; Percent complete: 63.1%; Average loss: 1.7538\n",
            "Iteration: 1263; Percent complete: 63.1%; Average loss: 1.8835\n",
            "Iteration: 1264; Percent complete: 63.2%; Average loss: 1.8690\n",
            "Iteration: 1265; Percent complete: 63.2%; Average loss: 1.9822\n",
            "Iteration: 1266; Percent complete: 63.3%; Average loss: 1.8335\n",
            "Iteration: 1267; Percent complete: 63.3%; Average loss: 2.0275\n",
            "Iteration: 1268; Percent complete: 63.4%; Average loss: 1.8381\n",
            "Iteration: 1269; Percent complete: 63.4%; Average loss: 1.9341\n",
            "Iteration: 1270; Percent complete: 63.5%; Average loss: 1.8476\n",
            "Iteration: 1271; Percent complete: 63.5%; Average loss: 1.9156\n",
            "Iteration: 1272; Percent complete: 63.6%; Average loss: 1.8932\n",
            "Iteration: 1273; Percent complete: 63.6%; Average loss: 1.9209\n",
            "Iteration: 1274; Percent complete: 63.7%; Average loss: 1.9058\n",
            "Iteration: 1275; Percent complete: 63.7%; Average loss: 2.0377\n",
            "Iteration: 1276; Percent complete: 63.8%; Average loss: 1.9071\n",
            "Iteration: 1277; Percent complete: 63.8%; Average loss: 1.9135\n",
            "Iteration: 1278; Percent complete: 63.9%; Average loss: 2.0050\n",
            "Iteration: 1279; Percent complete: 63.9%; Average loss: 1.9395\n",
            "Iteration: 1280; Percent complete: 64.0%; Average loss: 1.9608\n",
            "Iteration: 1281; Percent complete: 64.0%; Average loss: 1.9882\n",
            "Iteration: 1282; Percent complete: 64.1%; Average loss: 1.9154\n",
            "Iteration: 1283; Percent complete: 64.1%; Average loss: 1.9085\n",
            "Iteration: 1284; Percent complete: 64.2%; Average loss: 1.9541\n",
            "Iteration: 1285; Percent complete: 64.2%; Average loss: 1.9203\n",
            "Iteration: 1286; Percent complete: 64.3%; Average loss: 1.9900\n",
            "Iteration: 1287; Percent complete: 64.3%; Average loss: 1.9246\n",
            "Iteration: 1288; Percent complete: 64.4%; Average loss: 1.9048\n",
            "Iteration: 1289; Percent complete: 64.5%; Average loss: 1.7632\n",
            "Iteration: 1290; Percent complete: 64.5%; Average loss: 1.9486\n",
            "Iteration: 1291; Percent complete: 64.5%; Average loss: 1.9526\n",
            "Iteration: 1292; Percent complete: 64.6%; Average loss: 1.7196\n",
            "Iteration: 1293; Percent complete: 64.6%; Average loss: 1.8371\n",
            "Iteration: 1294; Percent complete: 64.7%; Average loss: 1.7440\n",
            "Iteration: 1295; Percent complete: 64.8%; Average loss: 1.7926\n",
            "Iteration: 1296; Percent complete: 64.8%; Average loss: 1.8545\n",
            "Iteration: 1297; Percent complete: 64.8%; Average loss: 1.8015\n",
            "Iteration: 1298; Percent complete: 64.9%; Average loss: 1.8389\n",
            "Iteration: 1299; Percent complete: 65.0%; Average loss: 1.8763\n",
            "Iteration: 1300; Percent complete: 65.0%; Average loss: 1.9424\n",
            "Iteration: 1301; Percent complete: 65.0%; Average loss: 1.8548\n",
            "Iteration: 1302; Percent complete: 65.1%; Average loss: 1.9099\n",
            "Iteration: 1303; Percent complete: 65.1%; Average loss: 1.8173\n",
            "Iteration: 1304; Percent complete: 65.2%; Average loss: 1.7901\n",
            "Iteration: 1305; Percent complete: 65.2%; Average loss: 1.7895\n",
            "Iteration: 1306; Percent complete: 65.3%; Average loss: 1.8577\n",
            "Iteration: 1307; Percent complete: 65.3%; Average loss: 1.7971\n",
            "Iteration: 1308; Percent complete: 65.4%; Average loss: 1.8848\n",
            "Iteration: 1309; Percent complete: 65.5%; Average loss: 1.8449\n",
            "Iteration: 1310; Percent complete: 65.5%; Average loss: 1.7980\n",
            "Iteration: 1311; Percent complete: 65.5%; Average loss: 1.8519\n",
            "Iteration: 1312; Percent complete: 65.6%; Average loss: 1.9543\n",
            "Iteration: 1313; Percent complete: 65.6%; Average loss: 1.8712\n",
            "Iteration: 1314; Percent complete: 65.7%; Average loss: 1.9077\n",
            "Iteration: 1315; Percent complete: 65.8%; Average loss: 1.8065\n",
            "Iteration: 1316; Percent complete: 65.8%; Average loss: 1.7286\n",
            "Iteration: 1317; Percent complete: 65.8%; Average loss: 1.7304\n",
            "Iteration: 1318; Percent complete: 65.9%; Average loss: 1.7786\n",
            "Iteration: 1319; Percent complete: 66.0%; Average loss: 1.8609\n",
            "Iteration: 1320; Percent complete: 66.0%; Average loss: 1.8552\n",
            "Iteration: 1321; Percent complete: 66.0%; Average loss: 1.8080\n",
            "Iteration: 1322; Percent complete: 66.1%; Average loss: 1.8895\n",
            "Iteration: 1323; Percent complete: 66.1%; Average loss: 1.7763\n",
            "Iteration: 1324; Percent complete: 66.2%; Average loss: 1.7324\n",
            "Iteration: 1325; Percent complete: 66.2%; Average loss: 1.9563\n",
            "Iteration: 1326; Percent complete: 66.3%; Average loss: 1.7804\n",
            "Iteration: 1327; Percent complete: 66.3%; Average loss: 1.9174\n",
            "Iteration: 1328; Percent complete: 66.4%; Average loss: 1.8014\n",
            "Iteration: 1329; Percent complete: 66.5%; Average loss: 1.8616\n",
            "Iteration: 1330; Percent complete: 66.5%; Average loss: 1.8141\n",
            "Iteration: 1331; Percent complete: 66.5%; Average loss: 1.9187\n",
            "Iteration: 1332; Percent complete: 66.6%; Average loss: 1.7998\n",
            "Iteration: 1333; Percent complete: 66.6%; Average loss: 1.8390\n",
            "Iteration: 1334; Percent complete: 66.7%; Average loss: 1.7287\n",
            "Iteration: 1335; Percent complete: 66.8%; Average loss: 1.7720\n",
            "Iteration: 1336; Percent complete: 66.8%; Average loss: 1.8011\n",
            "Iteration: 1337; Percent complete: 66.8%; Average loss: 1.8412\n",
            "Iteration: 1338; Percent complete: 66.9%; Average loss: 1.8322\n",
            "Iteration: 1339; Percent complete: 67.0%; Average loss: 1.7045\n",
            "Iteration: 1340; Percent complete: 67.0%; Average loss: 1.6039\n",
            "Iteration: 1341; Percent complete: 67.0%; Average loss: 1.7494\n",
            "Iteration: 1342; Percent complete: 67.1%; Average loss: 1.8907\n",
            "Iteration: 1343; Percent complete: 67.2%; Average loss: 1.7607\n",
            "Iteration: 1344; Percent complete: 67.2%; Average loss: 1.8591\n",
            "Iteration: 1345; Percent complete: 67.2%; Average loss: 1.7985\n",
            "Iteration: 1346; Percent complete: 67.3%; Average loss: 1.7697\n",
            "Iteration: 1347; Percent complete: 67.3%; Average loss: 1.8691\n",
            "Iteration: 1348; Percent complete: 67.4%; Average loss: 1.6363\n",
            "Iteration: 1349; Percent complete: 67.5%; Average loss: 1.7273\n",
            "Iteration: 1350; Percent complete: 67.5%; Average loss: 1.6517\n",
            "Iteration: 1351; Percent complete: 67.5%; Average loss: 1.7068\n",
            "Iteration: 1352; Percent complete: 67.6%; Average loss: 1.8009\n",
            "Iteration: 1353; Percent complete: 67.7%; Average loss: 1.8543\n",
            "Iteration: 1354; Percent complete: 67.7%; Average loss: 1.7467\n",
            "Iteration: 1355; Percent complete: 67.8%; Average loss: 1.6823\n",
            "Iteration: 1356; Percent complete: 67.8%; Average loss: 1.8166\n",
            "Iteration: 1357; Percent complete: 67.8%; Average loss: 1.7197\n",
            "Iteration: 1358; Percent complete: 67.9%; Average loss: 1.7164\n",
            "Iteration: 1359; Percent complete: 68.0%; Average loss: 1.8066\n",
            "Iteration: 1360; Percent complete: 68.0%; Average loss: 1.8479\n",
            "Iteration: 1361; Percent complete: 68.0%; Average loss: 1.5997\n",
            "Iteration: 1362; Percent complete: 68.1%; Average loss: 1.6898\n",
            "Iteration: 1363; Percent complete: 68.2%; Average loss: 1.8813\n",
            "Iteration: 1364; Percent complete: 68.2%; Average loss: 1.7663\n",
            "Iteration: 1365; Percent complete: 68.2%; Average loss: 1.7097\n",
            "Iteration: 1366; Percent complete: 68.3%; Average loss: 1.7968\n",
            "Iteration: 1367; Percent complete: 68.3%; Average loss: 1.6313\n",
            "Iteration: 1368; Percent complete: 68.4%; Average loss: 1.7502\n",
            "Iteration: 1369; Percent complete: 68.5%; Average loss: 1.7169\n",
            "Iteration: 1370; Percent complete: 68.5%; Average loss: 1.6348\n",
            "Iteration: 1371; Percent complete: 68.5%; Average loss: 1.8188\n",
            "Iteration: 1372; Percent complete: 68.6%; Average loss: 1.8192\n",
            "Iteration: 1373; Percent complete: 68.7%; Average loss: 1.6104\n",
            "Iteration: 1374; Percent complete: 68.7%; Average loss: 1.6770\n",
            "Iteration: 1375; Percent complete: 68.8%; Average loss: 1.8117\n",
            "Iteration: 1376; Percent complete: 68.8%; Average loss: 1.6453\n",
            "Iteration: 1377; Percent complete: 68.8%; Average loss: 1.7631\n",
            "Iteration: 1378; Percent complete: 68.9%; Average loss: 1.6461\n",
            "Iteration: 1379; Percent complete: 69.0%; Average loss: 1.7431\n",
            "Iteration: 1380; Percent complete: 69.0%; Average loss: 1.7833\n",
            "Iteration: 1381; Percent complete: 69.0%; Average loss: 1.8120\n",
            "Iteration: 1382; Percent complete: 69.1%; Average loss: 1.6591\n",
            "Iteration: 1383; Percent complete: 69.2%; Average loss: 1.7488\n",
            "Iteration: 1384; Percent complete: 69.2%; Average loss: 1.6693\n",
            "Iteration: 1385; Percent complete: 69.2%; Average loss: 1.6368\n",
            "Iteration: 1386; Percent complete: 69.3%; Average loss: 1.7506\n",
            "Iteration: 1387; Percent complete: 69.3%; Average loss: 1.6461\n",
            "Iteration: 1388; Percent complete: 69.4%; Average loss: 1.6900\n",
            "Iteration: 1389; Percent complete: 69.5%; Average loss: 1.6674\n",
            "Iteration: 1390; Percent complete: 69.5%; Average loss: 1.6162\n",
            "Iteration: 1391; Percent complete: 69.5%; Average loss: 1.7325\n",
            "Iteration: 1392; Percent complete: 69.6%; Average loss: 1.7231\n",
            "Iteration: 1393; Percent complete: 69.7%; Average loss: 1.7103\n",
            "Iteration: 1394; Percent complete: 69.7%; Average loss: 1.7430\n",
            "Iteration: 1395; Percent complete: 69.8%; Average loss: 1.6651\n",
            "Iteration: 1396; Percent complete: 69.8%; Average loss: 1.8113\n",
            "Iteration: 1397; Percent complete: 69.8%; Average loss: 1.6523\n",
            "Iteration: 1398; Percent complete: 69.9%; Average loss: 1.7296\n",
            "Iteration: 1399; Percent complete: 70.0%; Average loss: 1.5897\n",
            "Iteration: 1400; Percent complete: 70.0%; Average loss: 1.7414\n",
            "Iteration: 1401; Percent complete: 70.0%; Average loss: 1.7792\n",
            "Iteration: 1402; Percent complete: 70.1%; Average loss: 1.7435\n",
            "Iteration: 1403; Percent complete: 70.2%; Average loss: 1.6243\n",
            "Iteration: 1404; Percent complete: 70.2%; Average loss: 1.7250\n",
            "Iteration: 1405; Percent complete: 70.2%; Average loss: 1.7315\n",
            "Iteration: 1406; Percent complete: 70.3%; Average loss: 1.7306\n",
            "Iteration: 1407; Percent complete: 70.3%; Average loss: 1.5682\n",
            "Iteration: 1408; Percent complete: 70.4%; Average loss: 1.7646\n",
            "Iteration: 1409; Percent complete: 70.5%; Average loss: 1.8134\n",
            "Iteration: 1410; Percent complete: 70.5%; Average loss: 1.6411\n",
            "Iteration: 1411; Percent complete: 70.5%; Average loss: 1.6580\n",
            "Iteration: 1412; Percent complete: 70.6%; Average loss: 1.5537\n",
            "Iteration: 1413; Percent complete: 70.7%; Average loss: 1.6721\n",
            "Iteration: 1414; Percent complete: 70.7%; Average loss: 1.5633\n",
            "Iteration: 1415; Percent complete: 70.8%; Average loss: 1.7811\n",
            "Iteration: 1416; Percent complete: 70.8%; Average loss: 1.5940\n",
            "Iteration: 1417; Percent complete: 70.9%; Average loss: 1.6588\n",
            "Iteration: 1418; Percent complete: 70.9%; Average loss: 1.5062\n",
            "Iteration: 1419; Percent complete: 71.0%; Average loss: 1.6545\n",
            "Iteration: 1420; Percent complete: 71.0%; Average loss: 1.6663\n",
            "Iteration: 1421; Percent complete: 71.0%; Average loss: 1.6302\n",
            "Iteration: 1422; Percent complete: 71.1%; Average loss: 1.6916\n",
            "Iteration: 1423; Percent complete: 71.2%; Average loss: 1.6394\n",
            "Iteration: 1424; Percent complete: 71.2%; Average loss: 1.7301\n",
            "Iteration: 1425; Percent complete: 71.2%; Average loss: 1.6304\n",
            "Iteration: 1426; Percent complete: 71.3%; Average loss: 1.4394\n",
            "Iteration: 1427; Percent complete: 71.4%; Average loss: 1.5956\n",
            "Iteration: 1428; Percent complete: 71.4%; Average loss: 1.7663\n",
            "Iteration: 1429; Percent complete: 71.5%; Average loss: 1.6337\n",
            "Iteration: 1430; Percent complete: 71.5%; Average loss: 1.4699\n",
            "Iteration: 1431; Percent complete: 71.5%; Average loss: 1.5622\n",
            "Iteration: 1432; Percent complete: 71.6%; Average loss: 1.6167\n",
            "Iteration: 1433; Percent complete: 71.7%; Average loss: 1.6345\n",
            "Iteration: 1434; Percent complete: 71.7%; Average loss: 1.5819\n",
            "Iteration: 1435; Percent complete: 71.8%; Average loss: 1.6445\n",
            "Iteration: 1436; Percent complete: 71.8%; Average loss: 1.5876\n",
            "Iteration: 1437; Percent complete: 71.9%; Average loss: 1.7080\n",
            "Iteration: 1438; Percent complete: 71.9%; Average loss: 1.5375\n",
            "Iteration: 1439; Percent complete: 72.0%; Average loss: 1.6161\n",
            "Iteration: 1440; Percent complete: 72.0%; Average loss: 1.6970\n",
            "Iteration: 1441; Percent complete: 72.0%; Average loss: 1.5065\n",
            "Iteration: 1442; Percent complete: 72.1%; Average loss: 1.5612\n",
            "Iteration: 1443; Percent complete: 72.2%; Average loss: 1.5967\n",
            "Iteration: 1444; Percent complete: 72.2%; Average loss: 1.6003\n",
            "Iteration: 1445; Percent complete: 72.2%; Average loss: 1.5029\n",
            "Iteration: 1446; Percent complete: 72.3%; Average loss: 1.5820\n",
            "Iteration: 1447; Percent complete: 72.4%; Average loss: 1.5271\n",
            "Iteration: 1448; Percent complete: 72.4%; Average loss: 1.6371\n",
            "Iteration: 1449; Percent complete: 72.5%; Average loss: 1.6324\n",
            "Iteration: 1450; Percent complete: 72.5%; Average loss: 1.6472\n",
            "Iteration: 1451; Percent complete: 72.5%; Average loss: 1.6298\n",
            "Iteration: 1452; Percent complete: 72.6%; Average loss: 1.5945\n",
            "Iteration: 1453; Percent complete: 72.7%; Average loss: 1.5565\n",
            "Iteration: 1454; Percent complete: 72.7%; Average loss: 1.4941\n",
            "Iteration: 1455; Percent complete: 72.8%; Average loss: 1.6180\n",
            "Iteration: 1456; Percent complete: 72.8%; Average loss: 1.5874\n",
            "Iteration: 1457; Percent complete: 72.9%; Average loss: 1.5646\n",
            "Iteration: 1458; Percent complete: 72.9%; Average loss: 1.7471\n",
            "Iteration: 1459; Percent complete: 73.0%; Average loss: 1.4726\n",
            "Iteration: 1460; Percent complete: 73.0%; Average loss: 1.7044\n",
            "Iteration: 1461; Percent complete: 73.0%; Average loss: 1.5991\n",
            "Iteration: 1462; Percent complete: 73.1%; Average loss: 1.6333\n",
            "Iteration: 1463; Percent complete: 73.2%; Average loss: 1.5166\n",
            "Iteration: 1464; Percent complete: 73.2%; Average loss: 1.5468\n",
            "Iteration: 1465; Percent complete: 73.2%; Average loss: 1.7067\n",
            "Iteration: 1466; Percent complete: 73.3%; Average loss: 1.5621\n",
            "Iteration: 1467; Percent complete: 73.4%; Average loss: 1.6727\n",
            "Iteration: 1468; Percent complete: 73.4%; Average loss: 1.4677\n",
            "Iteration: 1469; Percent complete: 73.5%; Average loss: 1.7796\n",
            "Iteration: 1470; Percent complete: 73.5%; Average loss: 1.5759\n",
            "Iteration: 1471; Percent complete: 73.6%; Average loss: 1.6205\n",
            "Iteration: 1472; Percent complete: 73.6%; Average loss: 1.6378\n",
            "Iteration: 1473; Percent complete: 73.7%; Average loss: 1.5161\n",
            "Iteration: 1474; Percent complete: 73.7%; Average loss: 1.5792\n",
            "Iteration: 1475; Percent complete: 73.8%; Average loss: 1.6050\n",
            "Iteration: 1476; Percent complete: 73.8%; Average loss: 1.4810\n",
            "Iteration: 1477; Percent complete: 73.9%; Average loss: 1.5386\n",
            "Iteration: 1478; Percent complete: 73.9%; Average loss: 1.5413\n",
            "Iteration: 1479; Percent complete: 74.0%; Average loss: 1.6499\n",
            "Iteration: 1480; Percent complete: 74.0%; Average loss: 1.4500\n",
            "Iteration: 1481; Percent complete: 74.1%; Average loss: 1.6074\n",
            "Iteration: 1482; Percent complete: 74.1%; Average loss: 1.5886\n",
            "Iteration: 1483; Percent complete: 74.2%; Average loss: 1.6024\n",
            "Iteration: 1484; Percent complete: 74.2%; Average loss: 1.5583\n",
            "Iteration: 1485; Percent complete: 74.2%; Average loss: 1.4197\n",
            "Iteration: 1486; Percent complete: 74.3%; Average loss: 1.6084\n",
            "Iteration: 1487; Percent complete: 74.4%; Average loss: 1.5895\n",
            "Iteration: 1488; Percent complete: 74.4%; Average loss: 1.5586\n",
            "Iteration: 1489; Percent complete: 74.5%; Average loss: 1.5135\n",
            "Iteration: 1490; Percent complete: 74.5%; Average loss: 1.6739\n",
            "Iteration: 1491; Percent complete: 74.6%; Average loss: 1.4963\n",
            "Iteration: 1492; Percent complete: 74.6%; Average loss: 1.5980\n",
            "Iteration: 1493; Percent complete: 74.7%; Average loss: 1.5233\n",
            "Iteration: 1494; Percent complete: 74.7%; Average loss: 1.4709\n",
            "Iteration: 1495; Percent complete: 74.8%; Average loss: 1.6221\n",
            "Iteration: 1496; Percent complete: 74.8%; Average loss: 1.4918\n",
            "Iteration: 1497; Percent complete: 74.9%; Average loss: 1.6322\n",
            "Iteration: 1498; Percent complete: 74.9%; Average loss: 1.5943\n",
            "Iteration: 1499; Percent complete: 75.0%; Average loss: 1.5488\n",
            "Iteration: 1500; Percent complete: 75.0%; Average loss: 1.5918\n",
            "Iteration: 1501; Percent complete: 75.0%; Average loss: 1.5679\n",
            "Iteration: 1502; Percent complete: 75.1%; Average loss: 1.6255\n",
            "Iteration: 1503; Percent complete: 75.1%; Average loss: 1.4854\n",
            "Iteration: 1504; Percent complete: 75.2%; Average loss: 1.3773\n",
            "Iteration: 1505; Percent complete: 75.2%; Average loss: 1.5580\n",
            "Iteration: 1506; Percent complete: 75.3%; Average loss: 1.4954\n",
            "Iteration: 1507; Percent complete: 75.3%; Average loss: 1.4871\n",
            "Iteration: 1508; Percent complete: 75.4%; Average loss: 1.4877\n",
            "Iteration: 1509; Percent complete: 75.4%; Average loss: 1.4636\n",
            "Iteration: 1510; Percent complete: 75.5%; Average loss: 1.4223\n",
            "Iteration: 1511; Percent complete: 75.5%; Average loss: 1.4776\n",
            "Iteration: 1512; Percent complete: 75.6%; Average loss: 1.5851\n",
            "Iteration: 1513; Percent complete: 75.6%; Average loss: 1.5052\n",
            "Iteration: 1514; Percent complete: 75.7%; Average loss: 1.4689\n",
            "Iteration: 1515; Percent complete: 75.8%; Average loss: 1.5721\n",
            "Iteration: 1516; Percent complete: 75.8%; Average loss: 1.6181\n",
            "Iteration: 1517; Percent complete: 75.8%; Average loss: 1.4305\n",
            "Iteration: 1518; Percent complete: 75.9%; Average loss: 1.5330\n",
            "Iteration: 1519; Percent complete: 75.9%; Average loss: 1.3937\n",
            "Iteration: 1520; Percent complete: 76.0%; Average loss: 1.5735\n",
            "Iteration: 1521; Percent complete: 76.0%; Average loss: 1.5812\n",
            "Iteration: 1522; Percent complete: 76.1%; Average loss: 1.4836\n",
            "Iteration: 1523; Percent complete: 76.1%; Average loss: 1.5648\n",
            "Iteration: 1524; Percent complete: 76.2%; Average loss: 1.5155\n",
            "Iteration: 1525; Percent complete: 76.2%; Average loss: 1.6599\n",
            "Iteration: 1526; Percent complete: 76.3%; Average loss: 1.3848\n",
            "Iteration: 1527; Percent complete: 76.3%; Average loss: 1.4411\n",
            "Iteration: 1528; Percent complete: 76.4%; Average loss: 1.5081\n",
            "Iteration: 1529; Percent complete: 76.4%; Average loss: 1.5387\n",
            "Iteration: 1530; Percent complete: 76.5%; Average loss: 1.5535\n",
            "Iteration: 1531; Percent complete: 76.5%; Average loss: 1.6150\n",
            "Iteration: 1532; Percent complete: 76.6%; Average loss: 1.4401\n",
            "Iteration: 1533; Percent complete: 76.6%; Average loss: 1.4540\n",
            "Iteration: 1534; Percent complete: 76.7%; Average loss: 1.4689\n",
            "Iteration: 1535; Percent complete: 76.8%; Average loss: 1.4403\n",
            "Iteration: 1536; Percent complete: 76.8%; Average loss: 1.6000\n",
            "Iteration: 1537; Percent complete: 76.8%; Average loss: 1.4089\n",
            "Iteration: 1538; Percent complete: 76.9%; Average loss: 1.5242\n",
            "Iteration: 1539; Percent complete: 77.0%; Average loss: 1.4533\n",
            "Iteration: 1540; Percent complete: 77.0%; Average loss: 1.5960\n",
            "Iteration: 1541; Percent complete: 77.0%; Average loss: 1.5314\n",
            "Iteration: 1542; Percent complete: 77.1%; Average loss: 1.4048\n",
            "Iteration: 1543; Percent complete: 77.1%; Average loss: 1.4826\n",
            "Iteration: 1544; Percent complete: 77.2%; Average loss: 1.4189\n",
            "Iteration: 1545; Percent complete: 77.2%; Average loss: 1.5445\n",
            "Iteration: 1546; Percent complete: 77.3%; Average loss: 1.4024\n",
            "Iteration: 1547; Percent complete: 77.3%; Average loss: 1.5023\n",
            "Iteration: 1548; Percent complete: 77.4%; Average loss: 1.4752\n",
            "Iteration: 1549; Percent complete: 77.5%; Average loss: 1.5185\n",
            "Iteration: 1550; Percent complete: 77.5%; Average loss: 1.4624\n",
            "Iteration: 1551; Percent complete: 77.5%; Average loss: 1.4320\n",
            "Iteration: 1552; Percent complete: 77.6%; Average loss: 1.3851\n",
            "Iteration: 1553; Percent complete: 77.6%; Average loss: 1.4959\n",
            "Iteration: 1554; Percent complete: 77.7%; Average loss: 1.3310\n",
            "Iteration: 1555; Percent complete: 77.8%; Average loss: 1.4296\n",
            "Iteration: 1556; Percent complete: 77.8%; Average loss: 1.3707\n",
            "Iteration: 1557; Percent complete: 77.8%; Average loss: 1.5067\n",
            "Iteration: 1558; Percent complete: 77.9%; Average loss: 1.4255\n",
            "Iteration: 1559; Percent complete: 78.0%; Average loss: 1.4186\n",
            "Iteration: 1560; Percent complete: 78.0%; Average loss: 1.6442\n",
            "Iteration: 1561; Percent complete: 78.0%; Average loss: 1.4388\n",
            "Iteration: 1562; Percent complete: 78.1%; Average loss: 1.4837\n",
            "Iteration: 1563; Percent complete: 78.1%; Average loss: 1.4156\n",
            "Iteration: 1564; Percent complete: 78.2%; Average loss: 1.5090\n",
            "Iteration: 1565; Percent complete: 78.2%; Average loss: 1.3700\n",
            "Iteration: 1566; Percent complete: 78.3%; Average loss: 1.4172\n",
            "Iteration: 1567; Percent complete: 78.3%; Average loss: 1.3879\n",
            "Iteration: 1568; Percent complete: 78.4%; Average loss: 1.4505\n",
            "Iteration: 1569; Percent complete: 78.5%; Average loss: 1.3930\n",
            "Iteration: 1570; Percent complete: 78.5%; Average loss: 1.3543\n",
            "Iteration: 1571; Percent complete: 78.5%; Average loss: 1.4549\n",
            "Iteration: 1572; Percent complete: 78.6%; Average loss: 1.2810\n",
            "Iteration: 1573; Percent complete: 78.6%; Average loss: 1.3882\n",
            "Iteration: 1574; Percent complete: 78.7%; Average loss: 1.3684\n",
            "Iteration: 1575; Percent complete: 78.8%; Average loss: 1.5010\n",
            "Iteration: 1576; Percent complete: 78.8%; Average loss: 1.3611\n",
            "Iteration: 1577; Percent complete: 78.8%; Average loss: 1.5366\n",
            "Iteration: 1578; Percent complete: 78.9%; Average loss: 1.4934\n",
            "Iteration: 1579; Percent complete: 79.0%; Average loss: 1.4923\n",
            "Iteration: 1580; Percent complete: 79.0%; Average loss: 1.4583\n",
            "Iteration: 1581; Percent complete: 79.0%; Average loss: 1.4712\n",
            "Iteration: 1582; Percent complete: 79.1%; Average loss: 1.4346\n",
            "Iteration: 1583; Percent complete: 79.1%; Average loss: 1.3472\n",
            "Iteration: 1584; Percent complete: 79.2%; Average loss: 1.3415\n",
            "Iteration: 1585; Percent complete: 79.2%; Average loss: 1.4365\n",
            "Iteration: 1586; Percent complete: 79.3%; Average loss: 1.5110\n",
            "Iteration: 1587; Percent complete: 79.3%; Average loss: 1.4283\n",
            "Iteration: 1588; Percent complete: 79.4%; Average loss: 1.4913\n",
            "Iteration: 1589; Percent complete: 79.5%; Average loss: 1.2676\n",
            "Iteration: 1590; Percent complete: 79.5%; Average loss: 1.4464\n",
            "Iteration: 1591; Percent complete: 79.5%; Average loss: 1.3288\n",
            "Iteration: 1592; Percent complete: 79.6%; Average loss: 1.3693\n",
            "Iteration: 1593; Percent complete: 79.7%; Average loss: 1.2949\n",
            "Iteration: 1594; Percent complete: 79.7%; Average loss: 1.3589\n",
            "Iteration: 1595; Percent complete: 79.8%; Average loss: 1.3661\n",
            "Iteration: 1596; Percent complete: 79.8%; Average loss: 1.4078\n",
            "Iteration: 1597; Percent complete: 79.8%; Average loss: 1.5267\n",
            "Iteration: 1598; Percent complete: 79.9%; Average loss: 1.3293\n",
            "Iteration: 1599; Percent complete: 80.0%; Average loss: 1.5254\n",
            "Iteration: 1600; Percent complete: 80.0%; Average loss: 1.4616\n",
            "Iteration: 1601; Percent complete: 80.0%; Average loss: 1.3246\n",
            "Iteration: 1602; Percent complete: 80.1%; Average loss: 1.3124\n",
            "Iteration: 1603; Percent complete: 80.2%; Average loss: 1.4089\n",
            "Iteration: 1604; Percent complete: 80.2%; Average loss: 1.3889\n",
            "Iteration: 1605; Percent complete: 80.2%; Average loss: 1.3802\n",
            "Iteration: 1606; Percent complete: 80.3%; Average loss: 1.3481\n",
            "Iteration: 1607; Percent complete: 80.3%; Average loss: 1.3050\n",
            "Iteration: 1608; Percent complete: 80.4%; Average loss: 1.3179\n",
            "Iteration: 1609; Percent complete: 80.5%; Average loss: 1.3305\n",
            "Iteration: 1610; Percent complete: 80.5%; Average loss: 1.4800\n",
            "Iteration: 1611; Percent complete: 80.5%; Average loss: 1.4363\n",
            "Iteration: 1612; Percent complete: 80.6%; Average loss: 1.2826\n",
            "Iteration: 1613; Percent complete: 80.7%; Average loss: 1.3665\n",
            "Iteration: 1614; Percent complete: 80.7%; Average loss: 1.4362\n",
            "Iteration: 1615; Percent complete: 80.8%; Average loss: 1.2851\n",
            "Iteration: 1616; Percent complete: 80.8%; Average loss: 1.3944\n",
            "Iteration: 1617; Percent complete: 80.8%; Average loss: 1.1953\n",
            "Iteration: 1618; Percent complete: 80.9%; Average loss: 1.3389\n",
            "Iteration: 1619; Percent complete: 81.0%; Average loss: 1.3740\n",
            "Iteration: 1620; Percent complete: 81.0%; Average loss: 1.2684\n",
            "Iteration: 1621; Percent complete: 81.0%; Average loss: 1.3635\n",
            "Iteration: 1622; Percent complete: 81.1%; Average loss: 1.2901\n",
            "Iteration: 1623; Percent complete: 81.2%; Average loss: 1.4199\n",
            "Iteration: 1624; Percent complete: 81.2%; Average loss: 1.4056\n",
            "Iteration: 1625; Percent complete: 81.2%; Average loss: 1.3626\n",
            "Iteration: 1626; Percent complete: 81.3%; Average loss: 1.4358\n",
            "Iteration: 1627; Percent complete: 81.3%; Average loss: 1.2901\n",
            "Iteration: 1628; Percent complete: 81.4%; Average loss: 1.4066\n",
            "Iteration: 1629; Percent complete: 81.5%; Average loss: 1.3145\n",
            "Iteration: 1630; Percent complete: 81.5%; Average loss: 1.4382\n",
            "Iteration: 1631; Percent complete: 81.5%; Average loss: 1.2413\n",
            "Iteration: 1632; Percent complete: 81.6%; Average loss: 1.2909\n",
            "Iteration: 1633; Percent complete: 81.7%; Average loss: 1.3171\n",
            "Iteration: 1634; Percent complete: 81.7%; Average loss: 1.3781\n",
            "Iteration: 1635; Percent complete: 81.8%; Average loss: 1.2744\n",
            "Iteration: 1636; Percent complete: 81.8%; Average loss: 1.4475\n",
            "Iteration: 1637; Percent complete: 81.8%; Average loss: 1.2796\n",
            "Iteration: 1638; Percent complete: 81.9%; Average loss: 1.2436\n",
            "Iteration: 1639; Percent complete: 82.0%; Average loss: 1.3014\n",
            "Iteration: 1640; Percent complete: 82.0%; Average loss: 1.3193\n",
            "Iteration: 1641; Percent complete: 82.0%; Average loss: 1.3185\n",
            "Iteration: 1642; Percent complete: 82.1%; Average loss: 1.2293\n",
            "Iteration: 1643; Percent complete: 82.2%; Average loss: 1.3704\n",
            "Iteration: 1644; Percent complete: 82.2%; Average loss: 1.3010\n",
            "Iteration: 1645; Percent complete: 82.2%; Average loss: 1.3933\n",
            "Iteration: 1646; Percent complete: 82.3%; Average loss: 1.3387\n",
            "Iteration: 1647; Percent complete: 82.3%; Average loss: 1.4141\n",
            "Iteration: 1648; Percent complete: 82.4%; Average loss: 1.3774\n",
            "Iteration: 1649; Percent complete: 82.5%; Average loss: 1.3360\n",
            "Iteration: 1650; Percent complete: 82.5%; Average loss: 1.2689\n",
            "Iteration: 1651; Percent complete: 82.5%; Average loss: 1.2872\n",
            "Iteration: 1652; Percent complete: 82.6%; Average loss: 1.2501\n",
            "Iteration: 1653; Percent complete: 82.7%; Average loss: 1.2465\n",
            "Iteration: 1654; Percent complete: 82.7%; Average loss: 1.2079\n",
            "Iteration: 1655; Percent complete: 82.8%; Average loss: 1.1891\n",
            "Iteration: 1656; Percent complete: 82.8%; Average loss: 1.3611\n",
            "Iteration: 1657; Percent complete: 82.8%; Average loss: 1.2580\n",
            "Iteration: 1658; Percent complete: 82.9%; Average loss: 1.3572\n",
            "Iteration: 1659; Percent complete: 83.0%; Average loss: 1.2955\n",
            "Iteration: 1660; Percent complete: 83.0%; Average loss: 1.2289\n",
            "Iteration: 1661; Percent complete: 83.0%; Average loss: 1.2621\n",
            "Iteration: 1662; Percent complete: 83.1%; Average loss: 1.2753\n",
            "Iteration: 1663; Percent complete: 83.2%; Average loss: 1.2671\n",
            "Iteration: 1664; Percent complete: 83.2%; Average loss: 1.2428\n",
            "Iteration: 1665; Percent complete: 83.2%; Average loss: 1.2693\n",
            "Iteration: 1666; Percent complete: 83.3%; Average loss: 1.2947\n",
            "Iteration: 1667; Percent complete: 83.4%; Average loss: 1.2020\n",
            "Iteration: 1668; Percent complete: 83.4%; Average loss: 1.1351\n",
            "Iteration: 1669; Percent complete: 83.5%; Average loss: 1.3026\n",
            "Iteration: 1670; Percent complete: 83.5%; Average loss: 1.3078\n",
            "Iteration: 1671; Percent complete: 83.5%; Average loss: 1.2920\n",
            "Iteration: 1672; Percent complete: 83.6%; Average loss: 1.3083\n",
            "Iteration: 1673; Percent complete: 83.7%; Average loss: 1.3456\n",
            "Iteration: 1674; Percent complete: 83.7%; Average loss: 1.2849\n",
            "Iteration: 1675; Percent complete: 83.8%; Average loss: 1.2708\n",
            "Iteration: 1676; Percent complete: 83.8%; Average loss: 1.3832\n",
            "Iteration: 1677; Percent complete: 83.9%; Average loss: 1.2710\n",
            "Iteration: 1678; Percent complete: 83.9%; Average loss: 1.2932\n",
            "Iteration: 1679; Percent complete: 84.0%; Average loss: 1.1497\n",
            "Iteration: 1680; Percent complete: 84.0%; Average loss: 1.2687\n",
            "Iteration: 1681; Percent complete: 84.0%; Average loss: 1.3213\n",
            "Iteration: 1682; Percent complete: 84.1%; Average loss: 1.2729\n",
            "Iteration: 1683; Percent complete: 84.2%; Average loss: 1.2932\n",
            "Iteration: 1684; Percent complete: 84.2%; Average loss: 1.2206\n",
            "Iteration: 1685; Percent complete: 84.2%; Average loss: 1.2645\n",
            "Iteration: 1686; Percent complete: 84.3%; Average loss: 1.3322\n",
            "Iteration: 1687; Percent complete: 84.4%; Average loss: 1.1849\n",
            "Iteration: 1688; Percent complete: 84.4%; Average loss: 1.1974\n",
            "Iteration: 1689; Percent complete: 84.5%; Average loss: 1.1929\n",
            "Iteration: 1690; Percent complete: 84.5%; Average loss: 1.3079\n",
            "Iteration: 1691; Percent complete: 84.5%; Average loss: 1.2205\n",
            "Iteration: 1692; Percent complete: 84.6%; Average loss: 1.2525\n",
            "Iteration: 1693; Percent complete: 84.7%; Average loss: 1.2699\n",
            "Iteration: 1694; Percent complete: 84.7%; Average loss: 1.3082\n",
            "Iteration: 1695; Percent complete: 84.8%; Average loss: 1.3203\n",
            "Iteration: 1696; Percent complete: 84.8%; Average loss: 1.1751\n",
            "Iteration: 1697; Percent complete: 84.9%; Average loss: 1.1846\n",
            "Iteration: 1698; Percent complete: 84.9%; Average loss: 1.2241\n",
            "Iteration: 1699; Percent complete: 85.0%; Average loss: 1.2601\n",
            "Iteration: 1700; Percent complete: 85.0%; Average loss: 1.2461\n",
            "Iteration: 1701; Percent complete: 85.0%; Average loss: 1.1663\n",
            "Iteration: 1702; Percent complete: 85.1%; Average loss: 1.1506\n",
            "Iteration: 1703; Percent complete: 85.2%; Average loss: 1.2030\n",
            "Iteration: 1704; Percent complete: 85.2%; Average loss: 1.2982\n",
            "Iteration: 1705; Percent complete: 85.2%; Average loss: 1.1858\n",
            "Iteration: 1706; Percent complete: 85.3%; Average loss: 1.2070\n",
            "Iteration: 1707; Percent complete: 85.4%; Average loss: 1.3175\n",
            "Iteration: 1708; Percent complete: 85.4%; Average loss: 1.2098\n",
            "Iteration: 1709; Percent complete: 85.5%; Average loss: 1.1692\n",
            "Iteration: 1710; Percent complete: 85.5%; Average loss: 1.2123\n",
            "Iteration: 1711; Percent complete: 85.5%; Average loss: 1.1300\n",
            "Iteration: 1712; Percent complete: 85.6%; Average loss: 1.2032\n",
            "Iteration: 1713; Percent complete: 85.7%; Average loss: 1.2394\n",
            "Iteration: 1714; Percent complete: 85.7%; Average loss: 1.1425\n",
            "Iteration: 1715; Percent complete: 85.8%; Average loss: 1.2051\n",
            "Iteration: 1716; Percent complete: 85.8%; Average loss: 1.3230\n",
            "Iteration: 1717; Percent complete: 85.9%; Average loss: 1.2145\n",
            "Iteration: 1718; Percent complete: 85.9%; Average loss: 1.3289\n",
            "Iteration: 1719; Percent complete: 86.0%; Average loss: 1.1934\n",
            "Iteration: 1720; Percent complete: 86.0%; Average loss: 1.1830\n",
            "Iteration: 1721; Percent complete: 86.1%; Average loss: 1.3102\n",
            "Iteration: 1722; Percent complete: 86.1%; Average loss: 1.1138\n",
            "Iteration: 1723; Percent complete: 86.2%; Average loss: 1.0569\n",
            "Iteration: 1724; Percent complete: 86.2%; Average loss: 1.1838\n",
            "Iteration: 1725; Percent complete: 86.2%; Average loss: 1.1932\n",
            "Iteration: 1726; Percent complete: 86.3%; Average loss: 1.1738\n",
            "Iteration: 1727; Percent complete: 86.4%; Average loss: 1.2561\n",
            "Iteration: 1728; Percent complete: 86.4%; Average loss: 1.1802\n",
            "Iteration: 1729; Percent complete: 86.5%; Average loss: 1.1520\n",
            "Iteration: 1730; Percent complete: 86.5%; Average loss: 1.2090\n",
            "Iteration: 1731; Percent complete: 86.6%; Average loss: 1.2940\n",
            "Iteration: 1732; Percent complete: 86.6%; Average loss: 1.2342\n",
            "Iteration: 1733; Percent complete: 86.7%; Average loss: 1.2708\n",
            "Iteration: 1734; Percent complete: 86.7%; Average loss: 1.1178\n",
            "Iteration: 1735; Percent complete: 86.8%; Average loss: 1.1307\n",
            "Iteration: 1736; Percent complete: 86.8%; Average loss: 1.2098\n",
            "Iteration: 1737; Percent complete: 86.9%; Average loss: 1.2087\n",
            "Iteration: 1738; Percent complete: 86.9%; Average loss: 1.1605\n",
            "Iteration: 1739; Percent complete: 87.0%; Average loss: 1.2175\n",
            "Iteration: 1740; Percent complete: 87.0%; Average loss: 1.1536\n",
            "Iteration: 1741; Percent complete: 87.1%; Average loss: 1.1305\n",
            "Iteration: 1742; Percent complete: 87.1%; Average loss: 1.1686\n",
            "Iteration: 1743; Percent complete: 87.2%; Average loss: 1.1889\n",
            "Iteration: 1744; Percent complete: 87.2%; Average loss: 1.1761\n",
            "Iteration: 1745; Percent complete: 87.2%; Average loss: 1.1973\n",
            "Iteration: 1746; Percent complete: 87.3%; Average loss: 1.2011\n",
            "Iteration: 1747; Percent complete: 87.4%; Average loss: 1.1475\n",
            "Iteration: 1748; Percent complete: 87.4%; Average loss: 1.2096\n",
            "Iteration: 1749; Percent complete: 87.5%; Average loss: 1.2154\n",
            "Iteration: 1750; Percent complete: 87.5%; Average loss: 1.0798\n",
            "Iteration: 1751; Percent complete: 87.5%; Average loss: 1.1971\n",
            "Iteration: 1752; Percent complete: 87.6%; Average loss: 1.1038\n",
            "Iteration: 1753; Percent complete: 87.6%; Average loss: 1.0104\n",
            "Iteration: 1754; Percent complete: 87.7%; Average loss: 1.1428\n",
            "Iteration: 1755; Percent complete: 87.8%; Average loss: 1.2167\n",
            "Iteration: 1756; Percent complete: 87.8%; Average loss: 1.2030\n",
            "Iteration: 1757; Percent complete: 87.8%; Average loss: 1.0959\n",
            "Iteration: 1758; Percent complete: 87.9%; Average loss: 1.1152\n",
            "Iteration: 1759; Percent complete: 87.9%; Average loss: 1.0811\n",
            "Iteration: 1760; Percent complete: 88.0%; Average loss: 1.1333\n",
            "Iteration: 1761; Percent complete: 88.0%; Average loss: 1.1975\n",
            "Iteration: 1762; Percent complete: 88.1%; Average loss: 1.0604\n",
            "Iteration: 1763; Percent complete: 88.1%; Average loss: 1.1296\n",
            "Iteration: 1764; Percent complete: 88.2%; Average loss: 1.1440\n",
            "Iteration: 1765; Percent complete: 88.2%; Average loss: 1.1167\n",
            "Iteration: 1766; Percent complete: 88.3%; Average loss: 1.2077\n",
            "Iteration: 1767; Percent complete: 88.3%; Average loss: 1.1293\n",
            "Iteration: 1768; Percent complete: 88.4%; Average loss: 1.1154\n",
            "Iteration: 1769; Percent complete: 88.4%; Average loss: 1.1453\n",
            "Iteration: 1770; Percent complete: 88.5%; Average loss: 1.0328\n",
            "Iteration: 1771; Percent complete: 88.5%; Average loss: 1.1430\n",
            "Iteration: 1772; Percent complete: 88.6%; Average loss: 1.1247\n",
            "Iteration: 1773; Percent complete: 88.6%; Average loss: 1.0977\n",
            "Iteration: 1774; Percent complete: 88.7%; Average loss: 1.0388\n",
            "Iteration: 1775; Percent complete: 88.8%; Average loss: 1.1461\n",
            "Iteration: 1776; Percent complete: 88.8%; Average loss: 1.0788\n",
            "Iteration: 1777; Percent complete: 88.8%; Average loss: 1.0675\n",
            "Iteration: 1778; Percent complete: 88.9%; Average loss: 1.1051\n",
            "Iteration: 1779; Percent complete: 88.9%; Average loss: 1.0978\n",
            "Iteration: 1780; Percent complete: 89.0%; Average loss: 1.0619\n",
            "Iteration: 1781; Percent complete: 89.0%; Average loss: 1.0621\n",
            "Iteration: 1782; Percent complete: 89.1%; Average loss: 0.9915\n",
            "Iteration: 1783; Percent complete: 89.1%; Average loss: 1.1729\n",
            "Iteration: 1784; Percent complete: 89.2%; Average loss: 1.2109\n",
            "Iteration: 1785; Percent complete: 89.2%; Average loss: 1.1445\n",
            "Iteration: 1786; Percent complete: 89.3%; Average loss: 1.1745\n",
            "Iteration: 1787; Percent complete: 89.3%; Average loss: 1.1023\n",
            "Iteration: 1788; Percent complete: 89.4%; Average loss: 1.1729\n",
            "Iteration: 1789; Percent complete: 89.5%; Average loss: 1.2279\n",
            "Iteration: 1790; Percent complete: 89.5%; Average loss: 1.1717\n",
            "Iteration: 1791; Percent complete: 89.5%; Average loss: 1.1267\n",
            "Iteration: 1792; Percent complete: 89.6%; Average loss: 1.1684\n",
            "Iteration: 1793; Percent complete: 89.6%; Average loss: 1.2007\n",
            "Iteration: 1794; Percent complete: 89.7%; Average loss: 1.0641\n",
            "Iteration: 1795; Percent complete: 89.8%; Average loss: 1.0692\n",
            "Iteration: 1796; Percent complete: 89.8%; Average loss: 1.0803\n",
            "Iteration: 1797; Percent complete: 89.8%; Average loss: 1.1195\n",
            "Iteration: 1798; Percent complete: 89.9%; Average loss: 1.0616\n",
            "Iteration: 1799; Percent complete: 90.0%; Average loss: 0.9452\n",
            "Iteration: 1800; Percent complete: 90.0%; Average loss: 1.1895\n",
            "Iteration: 1801; Percent complete: 90.0%; Average loss: 1.0930\n",
            "Iteration: 1802; Percent complete: 90.1%; Average loss: 1.0816\n",
            "Iteration: 1803; Percent complete: 90.1%; Average loss: 1.1199\n",
            "Iteration: 1804; Percent complete: 90.2%; Average loss: 1.1150\n",
            "Iteration: 1805; Percent complete: 90.2%; Average loss: 1.0417\n",
            "Iteration: 1806; Percent complete: 90.3%; Average loss: 1.0812\n",
            "Iteration: 1807; Percent complete: 90.3%; Average loss: 1.1693\n",
            "Iteration: 1808; Percent complete: 90.4%; Average loss: 1.1345\n",
            "Iteration: 1809; Percent complete: 90.5%; Average loss: 1.1570\n",
            "Iteration: 1810; Percent complete: 90.5%; Average loss: 1.0715\n",
            "Iteration: 1811; Percent complete: 90.5%; Average loss: 1.1215\n",
            "Iteration: 1812; Percent complete: 90.6%; Average loss: 1.0243\n",
            "Iteration: 1813; Percent complete: 90.6%; Average loss: 1.0666\n",
            "Iteration: 1814; Percent complete: 90.7%; Average loss: 1.1638\n",
            "Iteration: 1815; Percent complete: 90.8%; Average loss: 1.1523\n",
            "Iteration: 1816; Percent complete: 90.8%; Average loss: 1.0765\n",
            "Iteration: 1817; Percent complete: 90.8%; Average loss: 1.1691\n",
            "Iteration: 1818; Percent complete: 90.9%; Average loss: 1.0435\n",
            "Iteration: 1819; Percent complete: 91.0%; Average loss: 0.9993\n",
            "Iteration: 1820; Percent complete: 91.0%; Average loss: 1.1178\n",
            "Iteration: 1821; Percent complete: 91.0%; Average loss: 1.0867\n",
            "Iteration: 1822; Percent complete: 91.1%; Average loss: 1.1807\n",
            "Iteration: 1823; Percent complete: 91.1%; Average loss: 1.0838\n",
            "Iteration: 1824; Percent complete: 91.2%; Average loss: 0.9926\n",
            "Iteration: 1825; Percent complete: 91.2%; Average loss: 1.0522\n",
            "Iteration: 1826; Percent complete: 91.3%; Average loss: 1.0440\n",
            "Iteration: 1827; Percent complete: 91.3%; Average loss: 1.0400\n",
            "Iteration: 1828; Percent complete: 91.4%; Average loss: 0.9976\n",
            "Iteration: 1829; Percent complete: 91.5%; Average loss: 1.0110\n",
            "Iteration: 1830; Percent complete: 91.5%; Average loss: 0.9543\n",
            "Iteration: 1831; Percent complete: 91.5%; Average loss: 1.0521\n",
            "Iteration: 1832; Percent complete: 91.6%; Average loss: 1.2007\n",
            "Iteration: 1833; Percent complete: 91.6%; Average loss: 1.0617\n",
            "Iteration: 1834; Percent complete: 91.7%; Average loss: 1.0936\n",
            "Iteration: 1835; Percent complete: 91.8%; Average loss: 1.1255\n",
            "Iteration: 1836; Percent complete: 91.8%; Average loss: 1.0848\n",
            "Iteration: 1837; Percent complete: 91.8%; Average loss: 0.9619\n",
            "Iteration: 1838; Percent complete: 91.9%; Average loss: 1.0781\n",
            "Iteration: 1839; Percent complete: 92.0%; Average loss: 1.1204\n",
            "Iteration: 1840; Percent complete: 92.0%; Average loss: 1.0432\n",
            "Iteration: 1841; Percent complete: 92.0%; Average loss: 1.0270\n",
            "Iteration: 1842; Percent complete: 92.1%; Average loss: 0.9740\n",
            "Iteration: 1843; Percent complete: 92.2%; Average loss: 1.1655\n",
            "Iteration: 1844; Percent complete: 92.2%; Average loss: 1.1601\n",
            "Iteration: 1845; Percent complete: 92.2%; Average loss: 1.1832\n",
            "Iteration: 1846; Percent complete: 92.3%; Average loss: 1.0239\n",
            "Iteration: 1847; Percent complete: 92.3%; Average loss: 0.9814\n",
            "Iteration: 1848; Percent complete: 92.4%; Average loss: 1.0318\n",
            "Iteration: 1849; Percent complete: 92.5%; Average loss: 1.0425\n",
            "Iteration: 1850; Percent complete: 92.5%; Average loss: 1.0670\n",
            "Iteration: 1851; Percent complete: 92.5%; Average loss: 0.9745\n",
            "Iteration: 1852; Percent complete: 92.6%; Average loss: 0.9820\n",
            "Iteration: 1853; Percent complete: 92.7%; Average loss: 1.1101\n",
            "Iteration: 1854; Percent complete: 92.7%; Average loss: 1.0834\n",
            "Iteration: 1855; Percent complete: 92.8%; Average loss: 1.0066\n",
            "Iteration: 1856; Percent complete: 92.8%; Average loss: 1.0451\n",
            "Iteration: 1857; Percent complete: 92.8%; Average loss: 1.1362\n",
            "Iteration: 1858; Percent complete: 92.9%; Average loss: 0.9858\n",
            "Iteration: 1859; Percent complete: 93.0%; Average loss: 1.0442\n",
            "Iteration: 1860; Percent complete: 93.0%; Average loss: 0.9911\n",
            "Iteration: 1861; Percent complete: 93.0%; Average loss: 0.9568\n",
            "Iteration: 1862; Percent complete: 93.1%; Average loss: 1.0655\n",
            "Iteration: 1863; Percent complete: 93.2%; Average loss: 1.0015\n",
            "Iteration: 1864; Percent complete: 93.2%; Average loss: 1.0312\n",
            "Iteration: 1865; Percent complete: 93.2%; Average loss: 0.9585\n",
            "Iteration: 1866; Percent complete: 93.3%; Average loss: 1.0091\n",
            "Iteration: 1867; Percent complete: 93.3%; Average loss: 0.9734\n",
            "Iteration: 1868; Percent complete: 93.4%; Average loss: 0.9650\n",
            "Iteration: 1869; Percent complete: 93.5%; Average loss: 1.0364\n",
            "Iteration: 1870; Percent complete: 93.5%; Average loss: 1.0279\n",
            "Iteration: 1871; Percent complete: 93.5%; Average loss: 1.0138\n",
            "Iteration: 1872; Percent complete: 93.6%; Average loss: 0.9340\n",
            "Iteration: 1873; Percent complete: 93.7%; Average loss: 1.0299\n",
            "Iteration: 1874; Percent complete: 93.7%; Average loss: 0.9840\n",
            "Iteration: 1875; Percent complete: 93.8%; Average loss: 0.9958\n",
            "Iteration: 1876; Percent complete: 93.8%; Average loss: 0.9021\n",
            "Iteration: 1877; Percent complete: 93.8%; Average loss: 0.9957\n",
            "Iteration: 1878; Percent complete: 93.9%; Average loss: 0.8785\n",
            "Iteration: 1879; Percent complete: 94.0%; Average loss: 1.0265\n",
            "Iteration: 1880; Percent complete: 94.0%; Average loss: 0.9396\n",
            "Iteration: 1881; Percent complete: 94.0%; Average loss: 0.9774\n",
            "Iteration: 1882; Percent complete: 94.1%; Average loss: 0.9514\n",
            "Iteration: 1883; Percent complete: 94.2%; Average loss: 0.9751\n",
            "Iteration: 1884; Percent complete: 94.2%; Average loss: 1.0104\n",
            "Iteration: 1885; Percent complete: 94.2%; Average loss: 0.9989\n",
            "Iteration: 1886; Percent complete: 94.3%; Average loss: 0.9982\n",
            "Iteration: 1887; Percent complete: 94.3%; Average loss: 0.9528\n",
            "Iteration: 1888; Percent complete: 94.4%; Average loss: 0.9839\n",
            "Iteration: 1889; Percent complete: 94.5%; Average loss: 0.8658\n",
            "Iteration: 1890; Percent complete: 94.5%; Average loss: 0.9284\n",
            "Iteration: 1891; Percent complete: 94.5%; Average loss: 0.9113\n",
            "Iteration: 1892; Percent complete: 94.6%; Average loss: 1.0316\n",
            "Iteration: 1893; Percent complete: 94.7%; Average loss: 1.0165\n",
            "Iteration: 1894; Percent complete: 94.7%; Average loss: 0.9293\n",
            "Iteration: 1895; Percent complete: 94.8%; Average loss: 0.9945\n",
            "Iteration: 1896; Percent complete: 94.8%; Average loss: 1.0114\n",
            "Iteration: 1897; Percent complete: 94.8%; Average loss: 0.8801\n",
            "Iteration: 1898; Percent complete: 94.9%; Average loss: 1.0183\n",
            "Iteration: 1899; Percent complete: 95.0%; Average loss: 0.9893\n",
            "Iteration: 1900; Percent complete: 95.0%; Average loss: 0.9835\n",
            "Iteration: 1901; Percent complete: 95.0%; Average loss: 0.9896\n",
            "Iteration: 1902; Percent complete: 95.1%; Average loss: 0.9699\n",
            "Iteration: 1903; Percent complete: 95.2%; Average loss: 0.9598\n",
            "Iteration: 1904; Percent complete: 95.2%; Average loss: 0.9818\n",
            "Iteration: 1905; Percent complete: 95.2%; Average loss: 1.0042\n",
            "Iteration: 1906; Percent complete: 95.3%; Average loss: 0.9652\n",
            "Iteration: 1907; Percent complete: 95.3%; Average loss: 0.9979\n",
            "Iteration: 1908; Percent complete: 95.4%; Average loss: 1.0341\n",
            "Iteration: 1909; Percent complete: 95.5%; Average loss: 1.0326\n",
            "Iteration: 1910; Percent complete: 95.5%; Average loss: 0.9880\n",
            "Iteration: 1911; Percent complete: 95.5%; Average loss: 0.9415\n",
            "Iteration: 1912; Percent complete: 95.6%; Average loss: 0.9522\n",
            "Iteration: 1913; Percent complete: 95.7%; Average loss: 0.9511\n",
            "Iteration: 1914; Percent complete: 95.7%; Average loss: 0.9709\n",
            "Iteration: 1915; Percent complete: 95.8%; Average loss: 1.0655\n",
            "Iteration: 1916; Percent complete: 95.8%; Average loss: 0.9717\n",
            "Iteration: 1917; Percent complete: 95.9%; Average loss: 0.9116\n",
            "Iteration: 1918; Percent complete: 95.9%; Average loss: 0.9630\n",
            "Iteration: 1919; Percent complete: 96.0%; Average loss: 0.9046\n",
            "Iteration: 1920; Percent complete: 96.0%; Average loss: 0.9042\n",
            "Iteration: 1921; Percent complete: 96.0%; Average loss: 0.8539\n",
            "Iteration: 1922; Percent complete: 96.1%; Average loss: 0.9496\n",
            "Iteration: 1923; Percent complete: 96.2%; Average loss: 0.9410\n",
            "Iteration: 1924; Percent complete: 96.2%; Average loss: 0.9103\n",
            "Iteration: 1925; Percent complete: 96.2%; Average loss: 0.9433\n",
            "Iteration: 1926; Percent complete: 96.3%; Average loss: 0.9298\n",
            "Iteration: 1927; Percent complete: 96.4%; Average loss: 0.9332\n",
            "Iteration: 1928; Percent complete: 96.4%; Average loss: 0.8764\n",
            "Iteration: 1929; Percent complete: 96.5%; Average loss: 0.9396\n",
            "Iteration: 1930; Percent complete: 96.5%; Average loss: 0.9382\n",
            "Iteration: 1931; Percent complete: 96.5%; Average loss: 0.9517\n",
            "Iteration: 1932; Percent complete: 96.6%; Average loss: 0.8881\n",
            "Iteration: 1933; Percent complete: 96.7%; Average loss: 0.9496\n",
            "Iteration: 1934; Percent complete: 96.7%; Average loss: 0.9094\n",
            "Iteration: 1935; Percent complete: 96.8%; Average loss: 0.8629\n",
            "Iteration: 1936; Percent complete: 96.8%; Average loss: 0.8860\n",
            "Iteration: 1937; Percent complete: 96.9%; Average loss: 0.9052\n",
            "Iteration: 1938; Percent complete: 96.9%; Average loss: 0.9642\n",
            "Iteration: 1939; Percent complete: 97.0%; Average loss: 0.9010\n",
            "Iteration: 1940; Percent complete: 97.0%; Average loss: 0.9078\n",
            "Iteration: 1941; Percent complete: 97.0%; Average loss: 0.8409\n",
            "Iteration: 1942; Percent complete: 97.1%; Average loss: 0.9487\n",
            "Iteration: 1943; Percent complete: 97.2%; Average loss: 0.8729\n",
            "Iteration: 1944; Percent complete: 97.2%; Average loss: 0.9349\n",
            "Iteration: 1945; Percent complete: 97.2%; Average loss: 0.8719\n",
            "Iteration: 1946; Percent complete: 97.3%; Average loss: 0.8819\n",
            "Iteration: 1947; Percent complete: 97.4%; Average loss: 1.0079\n",
            "Iteration: 1948; Percent complete: 97.4%; Average loss: 0.8424\n",
            "Iteration: 1949; Percent complete: 97.5%; Average loss: 0.7994\n",
            "Iteration: 1950; Percent complete: 97.5%; Average loss: 0.9905\n",
            "Iteration: 1951; Percent complete: 97.5%; Average loss: 0.9176\n",
            "Iteration: 1952; Percent complete: 97.6%; Average loss: 0.9530\n",
            "Iteration: 1953; Percent complete: 97.7%; Average loss: 0.8891\n",
            "Iteration: 1954; Percent complete: 97.7%; Average loss: 0.8761\n",
            "Iteration: 1955; Percent complete: 97.8%; Average loss: 0.8683\n",
            "Iteration: 1956; Percent complete: 97.8%; Average loss: 0.8775\n",
            "Iteration: 1957; Percent complete: 97.9%; Average loss: 0.8341\n",
            "Iteration: 1958; Percent complete: 97.9%; Average loss: 0.9053\n",
            "Iteration: 1959; Percent complete: 98.0%; Average loss: 0.8912\n",
            "Iteration: 1960; Percent complete: 98.0%; Average loss: 0.9220\n",
            "Iteration: 1961; Percent complete: 98.0%; Average loss: 0.9832\n",
            "Iteration: 1962; Percent complete: 98.1%; Average loss: 0.9266\n",
            "Iteration: 1963; Percent complete: 98.2%; Average loss: 0.9780\n",
            "Iteration: 1964; Percent complete: 98.2%; Average loss: 0.8795\n",
            "Iteration: 1965; Percent complete: 98.2%; Average loss: 0.8366\n",
            "Iteration: 1966; Percent complete: 98.3%; Average loss: 0.8750\n",
            "Iteration: 1967; Percent complete: 98.4%; Average loss: 0.9034\n",
            "Iteration: 1968; Percent complete: 98.4%; Average loss: 0.8662\n",
            "Iteration: 1969; Percent complete: 98.5%; Average loss: 0.8384\n",
            "Iteration: 1970; Percent complete: 98.5%; Average loss: 0.8666\n",
            "Iteration: 1971; Percent complete: 98.6%; Average loss: 0.8795\n",
            "Iteration: 1972; Percent complete: 98.6%; Average loss: 0.9004\n",
            "Iteration: 1973; Percent complete: 98.7%; Average loss: 0.8945\n",
            "Iteration: 1974; Percent complete: 98.7%; Average loss: 0.9300\n",
            "Iteration: 1975; Percent complete: 98.8%; Average loss: 0.8331\n",
            "Iteration: 1976; Percent complete: 98.8%; Average loss: 0.8870\n",
            "Iteration: 1977; Percent complete: 98.9%; Average loss: 0.8630\n",
            "Iteration: 1978; Percent complete: 98.9%; Average loss: 0.9303\n",
            "Iteration: 1979; Percent complete: 99.0%; Average loss: 0.9419\n",
            "Iteration: 1980; Percent complete: 99.0%; Average loss: 0.8501\n",
            "Iteration: 1981; Percent complete: 99.1%; Average loss: 0.9118\n",
            "Iteration: 1982; Percent complete: 99.1%; Average loss: 0.9013\n",
            "Iteration: 1983; Percent complete: 99.2%; Average loss: 0.9444\n",
            "Iteration: 1984; Percent complete: 99.2%; Average loss: 0.8664\n",
            "Iteration: 1985; Percent complete: 99.2%; Average loss: 0.8330\n",
            "Iteration: 1986; Percent complete: 99.3%; Average loss: 0.8630\n",
            "Iteration: 1987; Percent complete: 99.4%; Average loss: 0.9125\n",
            "Iteration: 1988; Percent complete: 99.4%; Average loss: 0.9022\n",
            "Iteration: 1989; Percent complete: 99.5%; Average loss: 0.9114\n",
            "Iteration: 1990; Percent complete: 99.5%; Average loss: 0.9020\n",
            "Iteration: 1991; Percent complete: 99.6%; Average loss: 0.8741\n",
            "Iteration: 1992; Percent complete: 99.6%; Average loss: 0.9064\n",
            "Iteration: 1993; Percent complete: 99.7%; Average loss: 0.8697\n",
            "Iteration: 1994; Percent complete: 99.7%; Average loss: 0.8997\n",
            "Iteration: 1995; Percent complete: 99.8%; Average loss: 0.8394\n",
            "Iteration: 1996; Percent complete: 99.8%; Average loss: 0.9182\n",
            "Iteration: 1997; Percent complete: 99.9%; Average loss: 0.8372\n",
            "Iteration: 1998; Percent complete: 99.9%; Average loss: 0.8830\n",
            "Iteration: 1999; Percent complete: 100.0%; Average loss: 0.8507\n",
            "Iteration: 2000; Percent complete: 100.0%; Average loss: 0.8173\n"
          ]
        }
      ],
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.00005\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 2000\n",
        "print_every = 1\n",
        "save_every = 2000\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt06KZTsEwam",
        "outputId": "c9134381-9f37-434a-9bf9-f9ef504b8d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> hi\n",
            "Bot: well you re going to make a good deal . . . . . .\n",
            "> how are you\n",
            "Bot: i m throwing a party on friday . s all you . . . .\n",
            "> am I invited?\n",
            "Bot: i don t have a long time . . . . . . .\n",
            "> okay then\n",
            "Bot: you re right . i don t want to go back in . . . .\n",
            "> it will be so much fun\n",
            "Bot: it wouldn t be nice if it wasn t be more . . . . .\n",
            "> yes\n",
            "Bot: you can t even say that ? . . . . . .\n",
            "> no\n",
            "Bot: you might need to tell me . you don t want to come over ? . .\n",
            "> no, i want to\n",
            "Bot: that s a good idea . . . . . . . .\n",
            "> quit\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XCDjTAboGpdf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}